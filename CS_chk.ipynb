{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS chk.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLQ91jz/dL5ErO10ZiJBM8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK0V2hAYiK5m",
        "outputId": "9e0b1ead-bc98-4e70-9df1-51401ac75a8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pickle # saving and loading trained model\n",
        "from os import path\n",
        "\n",
        "# importing required libraries for normalizing data\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# importing library for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# importing library for support vector machine classifier\n",
        "from sklearn.svm import SVC\n",
        "# importing library for K-neares-neighbor classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# importing library for Linear Discriminant Analysis Model\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# importing library for Quadratic Discriminant Analysis Model\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score # for calculating accuracy of model\n",
        "from sklearn.model_selection import train_test_split # for splitting the dataset for training and testing\n",
        "from sklearn.metrics import classification_report # for generating a classification report of model\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "from keras.layers import Dense # importing dense layer\n",
        "from keras.models import Sequential #importing Sequential layer\n",
        "from keras.models import model_from_json # saving and loading trained model\n",
        "\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "\n",
        "# representation of model layers\n",
        "from keras.utils.vis_utils import plot_model"
      ],
      "metadata": {
        "id": "-CuI5o-AiLh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
        "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
        "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
        "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
        "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
        "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
        "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
        "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\",\"difficulty_level\"]"
      ],
      "metadata": {
        "id": "kn9YIoKgiWSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/cyberData/KDDTrain+.txt',header=None, names=col_names)\n",
        "data_t = pd.read_table(\"/content/drive/MyDrive/cyberData/KDDTest+.txt\",header=None, names=col_names)"
      ],
      "metadata": {
        "id": "TC93MT7lic6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "XcGADzivim3p",
        "outputId": "cc958ae6-c09f-4c5b-dd37-c00ad41363f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        duration protocol_type   service flag  src_bytes  dst_bytes  land  \\\n",
              "0              0           tcp  ftp_data   SF        491          0     0   \n",
              "1              0           udp     other   SF        146          0     0   \n",
              "2              0           tcp   private   S0          0          0     0   \n",
              "3              0           tcp      http   SF        232       8153     0   \n",
              "4              0           tcp      http   SF        199        420     0   \n",
              "...          ...           ...       ...  ...        ...        ...   ...   \n",
              "125968         0           tcp   private   S0          0          0     0   \n",
              "125969         8           udp   private   SF        105        145     0   \n",
              "125970         0           tcp      smtp   SF       2231        384     0   \n",
              "125971         0           tcp    klogin   S0          0          0     0   \n",
              "125972         0           tcp  ftp_data   SF        151          0     0   \n",
              "\n",
              "        wrong_fragment  urgent  hot  ...  dst_host_same_srv_rate  \\\n",
              "0                    0       0    0  ...                    0.17   \n",
              "1                    0       0    0  ...                    0.00   \n",
              "2                    0       0    0  ...                    0.10   \n",
              "3                    0       0    0  ...                    1.00   \n",
              "4                    0       0    0  ...                    1.00   \n",
              "...                ...     ...  ...  ...                     ...   \n",
              "125968               0       0    0  ...                    0.10   \n",
              "125969               0       0    0  ...                    0.96   \n",
              "125970               0       0    0  ...                    0.12   \n",
              "125971               0       0    0  ...                    0.03   \n",
              "125972               0       0    0  ...                    0.30   \n",
              "\n",
              "        dst_host_diff_srv_rate  dst_host_same_src_port_rate  \\\n",
              "0                         0.03                         0.17   \n",
              "1                         0.60                         0.88   \n",
              "2                         0.05                         0.00   \n",
              "3                         0.00                         0.03   \n",
              "4                         0.00                         0.00   \n",
              "...                        ...                          ...   \n",
              "125968                    0.06                         0.00   \n",
              "125969                    0.01                         0.01   \n",
              "125970                    0.06                         0.00   \n",
              "125971                    0.05                         0.00   \n",
              "125972                    0.03                         0.30   \n",
              "\n",
              "        dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
              "0                              0.00                  0.00   \n",
              "1                              0.00                  0.00   \n",
              "2                              0.00                  1.00   \n",
              "3                              0.04                  0.03   \n",
              "4                              0.00                  0.00   \n",
              "...                             ...                   ...   \n",
              "125968                         0.00                  1.00   \n",
              "125969                         0.00                  0.00   \n",
              "125970                         0.00                  0.72   \n",
              "125971                         0.00                  1.00   \n",
              "125972                         0.00                  0.00   \n",
              "\n",
              "        dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
              "0                           0.00                  0.05   \n",
              "1                           0.00                  0.00   \n",
              "2                           1.00                  0.00   \n",
              "3                           0.01                  0.00   \n",
              "4                           0.00                  0.00   \n",
              "...                          ...                   ...   \n",
              "125968                      1.00                  0.00   \n",
              "125969                      0.00                  0.00   \n",
              "125970                      0.00                  0.01   \n",
              "125971                      1.00                  0.00   \n",
              "125972                      0.00                  0.00   \n",
              "\n",
              "        dst_host_srv_rerror_rate    label  difficulty_level  \n",
              "0                           0.00   normal                20  \n",
              "1                           0.00   normal                15  \n",
              "2                           0.00  neptune                19  \n",
              "3                           0.01   normal                21  \n",
              "4                           0.00   normal                21  \n",
              "...                          ...      ...               ...  \n",
              "125968                      0.00  neptune                20  \n",
              "125969                      0.00   normal                21  \n",
              "125970                      0.00   normal                18  \n",
              "125971                      0.00  neptune                20  \n",
              "125972                      0.00   normal                21  \n",
              "\n",
              "[125973 rows x 43 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2b7d5301-d8ac-4093-9229-2cfca3f5ce27\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>duration</th>\n",
              "      <th>protocol_type</th>\n",
              "      <th>service</th>\n",
              "      <th>flag</th>\n",
              "      <th>src_bytes</th>\n",
              "      <th>dst_bytes</th>\n",
              "      <th>land</th>\n",
              "      <th>wrong_fragment</th>\n",
              "      <th>urgent</th>\n",
              "      <th>hot</th>\n",
              "      <th>...</th>\n",
              "      <th>dst_host_same_srv_rate</th>\n",
              "      <th>dst_host_diff_srv_rate</th>\n",
              "      <th>dst_host_same_src_port_rate</th>\n",
              "      <th>dst_host_srv_diff_host_rate</th>\n",
              "      <th>dst_host_serror_rate</th>\n",
              "      <th>dst_host_srv_serror_rate</th>\n",
              "      <th>dst_host_rerror_rate</th>\n",
              "      <th>dst_host_srv_rerror_rate</th>\n",
              "      <th>label</th>\n",
              "      <th>difficulty_level</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>ftp_data</td>\n",
              "      <td>SF</td>\n",
              "      <td>491</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>udp</td>\n",
              "      <td>other</td>\n",
              "      <td>SF</td>\n",
              "      <td>146</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>private</td>\n",
              "      <td>S0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>neptune</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>232</td>\n",
              "      <td>8153</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>normal</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>199</td>\n",
              "      <td>420</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125968</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>private</td>\n",
              "      <td>S0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>neptune</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125969</th>\n",
              "      <td>8</td>\n",
              "      <td>udp</td>\n",
              "      <td>private</td>\n",
              "      <td>SF</td>\n",
              "      <td>105</td>\n",
              "      <td>145</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125970</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>smtp</td>\n",
              "      <td>SF</td>\n",
              "      <td>2231</td>\n",
              "      <td>384</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125971</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>klogin</td>\n",
              "      <td>S0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>neptune</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125972</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>ftp_data</td>\n",
              "      <td>SF</td>\n",
              "      <td>151</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125973 rows Ã— 43 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b7d5301-d8ac-4093-9229-2cfca3f5ce27')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2b7d5301-d8ac-4093-9229-2cfca3f5ce27 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2b7d5301-d8ac-4093-9229-2cfca3f5ce27');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove attribute 'difficulty_level'\n",
        "data.drop(['difficulty_level'],axis=1,inplace=True)\n",
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VycFUFSJiscx",
        "outputId": "4ec8e9e5-ca99-462a-8788-4e8f0054b21e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(125973, 42)"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove attribute 'difficulty_level'\n",
        "data_t.drop(['difficulty_level'],axis=1,inplace=True)\n",
        "data_t.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz5fTrfUtc0y",
        "outputId": "01be57ce-6cd5-489f-9fad-730aed7e90cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22544, 42)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of attack labels \n",
        "data['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA_OzN4Mivuy",
        "outputId": "860502f3-e8be-41d7-b8ea-cc7e5fc95d99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "normal             67343\n",
              "neptune            41214\n",
              "satan               3633\n",
              "ipsweep             3599\n",
              "portsweep           2931\n",
              "smurf               2646\n",
              "nmap                1493\n",
              "back                 956\n",
              "teardrop             892\n",
              "warezclient          890\n",
              "pod                  201\n",
              "guess_passwd          53\n",
              "buffer_overflow       30\n",
              "warezmaster           20\n",
              "land                  18\n",
              "imap                  11\n",
              "rootkit               10\n",
              "loadmodule             9\n",
              "ftp_write              8\n",
              "multihop               7\n",
              "phf                    4\n",
              "perl                   3\n",
              "spy                    2\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# changing attack labels to their respective attack class\n",
        "def change_label(df):\n",
        "  df.label.replace(['apache2','back','land','neptune','mailbomb','pod','processtable','smurf','teardrop','udpstorm','worm'],'Dos',inplace=True)\n",
        "  df.label.replace(['ftp_write','guess_passwd','httptunnel','imap','multihop','named','phf','sendmail',\n",
        "       'snmpgetattack','snmpguess','spy','warezclient','warezmaster','xlock','xsnoop'],'R2L',inplace=True)\n",
        "  df.label.replace(['ipsweep','mscan','nmap','portsweep','saint','satan'],'Probe',inplace=True)\n",
        "  df.label.replace(['buffer_overflow','loadmodule','perl','ps','rootkit','sqlattack','xterm'],'U2R',inplace=True)"
      ],
      "metadata": {
        "id": "f6YGACjHi1B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling change_label() function\n",
        "change_label(data)"
      ],
      "metadata": {
        "id": "0BAxhR3bi3Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "change_label(data_t)"
      ],
      "metadata": {
        "id": "psFvUXattjCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution of attack classes\n",
        "data.label.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUJ143f_i5zB",
        "outputId": "5d07bdad-3c6a-4017-9304-cad8954a5b2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "normal    67343\n",
              "Dos       45927\n",
              "Probe     11656\n",
              "R2L         995\n",
              "U2R          52\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lHb0lCOlsrvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i7EEDIGGsrrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tCdlKndesroi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sbJizBwssrl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pbYASh8bsrjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selecting numeric attributes columns from data\n",
        "numeric_col = data.select_dtypes(include='number').columns"
      ],
      "metadata": {
        "id": "KMNtq-Abi8QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using standard scaler for normalizing\n",
        "std_scaler = StandardScaler()\n",
        "def normalization(df,col):\n",
        "  for i in col:\n",
        "    arr = df[i]\n",
        "    arr = np.array(arr)\n",
        "    df[i] = std_scaler.fit_transform(arr.reshape(len(arr),1))\n",
        "  return df"
      ],
      "metadata": {
        "id": "F3BblrnTi-cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data before normalization\n",
        "data.head()\n",
        "\n",
        "# calling the normalization() function\n",
        "data = normalization(data.copy(),numeric_col)\n",
        "\n",
        "# data after normalization\n",
        "data.head()\n",
        "\n",
        "# selecting categorical data attributes\n",
        "cat_col = ['protocol_type','service','flag']\n",
        "\n",
        "# creating a dataframe with only categorical attributes\n",
        "categorical = data[cat_col]\n",
        "categorical.head()\n",
        "\n",
        "# one-h\n",
        "categorical = pd.get_dummies(categorical,columns=cat_col)\n",
        "categorical.head()\n",
        "\n",
        "# changing attack labels into two categories 'normal' and 'abnormal'\n",
        "bin_label = pd.DataFrame(data.label.map(lambda x:'normal' if x=='normal' else 'abnormal'))\n",
        "\n",
        "# creating a dataframe with binary labels (normal,abnormal)\n",
        "bin_data = data.copy()\n",
        "bin_data['label'] = bin_label\n",
        "\n",
        "# label encoding (0,1) binary labels (abnormal,normal)\n",
        "le1 = preprocessing.LabelEncoder()\n",
        "enc_label = bin_label.apply(le1.fit_transform)\n",
        "bin_data['intrusion'] = enc_label\n",
        "\n",
        "# dataset with binary labels and label encoded column\n",
        "bin_data.head()\n",
        "\n",
        "# one-hot-encoding attack label\n",
        "bin_data = pd.get_dummies(bin_data,columns=['label'],prefix=\"\",prefix_sep=\"\") \n",
        "bin_data['label'] = bin_label\n",
        "bin_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "EOxfdQa6jInb",
        "outputId": "aafd4d51-62fc-454c-d92f-0d1be2a286dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        duration protocol_type   service flag  src_bytes  dst_bytes      land  \\\n",
              "0      -0.110249           tcp  ftp_data   SF  -0.007679  -0.004919 -0.014089   \n",
              "1      -0.110249           udp     other   SF  -0.007737  -0.004919 -0.014089   \n",
              "2      -0.110249           tcp   private   S0  -0.007762  -0.004919 -0.014089   \n",
              "3      -0.110249           tcp      http   SF  -0.007723  -0.002891 -0.014089   \n",
              "4      -0.110249           tcp      http   SF  -0.007728  -0.004814 -0.014089   \n",
              "...          ...           ...       ...  ...        ...        ...       ...   \n",
              "125968 -0.110249           tcp   private   S0  -0.007762  -0.004919 -0.014089   \n",
              "125969 -0.107178           udp   private   SF  -0.007744  -0.004883 -0.014089   \n",
              "125970 -0.110249           tcp      smtp   SF  -0.007382  -0.004823 -0.014089   \n",
              "125971 -0.110249           tcp    klogin   S0  -0.007762  -0.004919 -0.014089   \n",
              "125972 -0.110249           tcp  ftp_data   SF  -0.007737  -0.004919 -0.014089   \n",
              "\n",
              "        wrong_fragment    urgent       hot  ...  dst_host_same_src_port_rate  \\\n",
              "0            -0.089486 -0.007736 -0.095076  ...                     0.069972   \n",
              "1            -0.089486 -0.007736 -0.095076  ...                     2.367737   \n",
              "2            -0.089486 -0.007736 -0.095076  ...                    -0.480197   \n",
              "3            -0.089486 -0.007736 -0.095076  ...                    -0.383108   \n",
              "4            -0.089486 -0.007736 -0.095076  ...                    -0.480197   \n",
              "...                ...       ...       ...  ...                          ...   \n",
              "125968       -0.089486 -0.007736 -0.095076  ...                    -0.480197   \n",
              "125969       -0.089486 -0.007736 -0.095076  ...                    -0.447834   \n",
              "125970       -0.089486 -0.007736 -0.095076  ...                    -0.480197   \n",
              "125971       -0.089486 -0.007736 -0.095076  ...                    -0.480197   \n",
              "125972       -0.089486 -0.007736 -0.095076  ...                     0.490690   \n",
              "\n",
              "        dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
              "0                         -0.289103             -0.639532   \n",
              "1                         -0.289103             -0.639532   \n",
              "2                         -0.289103              1.608759   \n",
              "3                          0.066252             -0.572083   \n",
              "4                         -0.289103             -0.639532   \n",
              "...                             ...                   ...   \n",
              "125968                    -0.289103              1.608759   \n",
              "125969                    -0.289103             -0.639532   \n",
              "125970                    -0.289103              0.979238   \n",
              "125971                    -0.289103              1.608759   \n",
              "125972                    -0.289103             -0.639532   \n",
              "\n",
              "        dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
              "0                      -0.624871             -0.224532   \n",
              "1                      -0.624871             -0.387635   \n",
              "2                       1.618955             -0.387635   \n",
              "3                      -0.602433             -0.387635   \n",
              "4                      -0.624871             -0.387635   \n",
              "...                          ...                   ...   \n",
              "125968                  1.618955             -0.387635   \n",
              "125969                 -0.624871             -0.387635   \n",
              "125970                 -0.624871             -0.355014   \n",
              "125971                  1.618955             -0.387635   \n",
              "125972                 -0.624871             -0.387635   \n",
              "\n",
              "        dst_host_srv_rerror_rate  intrusion  abnormal  normal     label  \n",
              "0                      -0.376387          1         0       1    normal  \n",
              "1                      -0.376387          1         0       1    normal  \n",
              "2                      -0.376387          0         1       0  abnormal  \n",
              "3                      -0.345084          1         0       1    normal  \n",
              "4                      -0.376387          1         0       1    normal  \n",
              "...                          ...        ...       ...     ...       ...  \n",
              "125968                 -0.376387          0         1       0  abnormal  \n",
              "125969                 -0.376387          1         0       1    normal  \n",
              "125970                 -0.376387          1         0       1    normal  \n",
              "125971                 -0.376387          0         1       0  abnormal  \n",
              "125972                 -0.376387          1         0       1    normal  \n",
              "\n",
              "[125973 rows x 45 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0388b315-36ba-4da3-91f6-655356fb5e42\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>duration</th>\n",
              "      <th>protocol_type</th>\n",
              "      <th>service</th>\n",
              "      <th>flag</th>\n",
              "      <th>src_bytes</th>\n",
              "      <th>dst_bytes</th>\n",
              "      <th>land</th>\n",
              "      <th>wrong_fragment</th>\n",
              "      <th>urgent</th>\n",
              "      <th>hot</th>\n",
              "      <th>...</th>\n",
              "      <th>dst_host_same_src_port_rate</th>\n",
              "      <th>dst_host_srv_diff_host_rate</th>\n",
              "      <th>dst_host_serror_rate</th>\n",
              "      <th>dst_host_srv_serror_rate</th>\n",
              "      <th>dst_host_rerror_rate</th>\n",
              "      <th>dst_host_srv_rerror_rate</th>\n",
              "      <th>intrusion</th>\n",
              "      <th>abnormal</th>\n",
              "      <th>normal</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.110249</td>\n",
              "      <td>tcp</td>\n",
              "      <td>ftp_data</td>\n",
              "      <td>SF</td>\n",
              "      <td>-0.007679</td>\n",
              "      <td>-0.004919</td>\n",
              "      <td>-0.014089</td>\n",
              "      <td>-0.089486</td>\n",
              "      <td>-0.007736</td>\n",
              "      <td>-0.095076</td>\n",
              "      <td>...</td>\n",
              "      <td>0.069972</td>\n",
              "      <td>-0.289103</td>\n",
              "      <td>-0.639532</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>-0.224532</td>\n",
              "      <td>-0.376387</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.110249</td>\n",
              "      <td>udp</td>\n",
              "      <td>other</td>\n",
              "      <td>SF</td>\n",
              "      <td>-0.007737</td>\n",
              "      <td>-0.004919</td>\n",
              "      <td>-0.014089</td>\n",
              "      <td>-0.089486</td>\n",
              "      <td>-0.007736</td>\n",
              "      <td>-0.095076</td>\n",
              "      <td>...</td>\n",
              "      <td>2.367737</td>\n",
              "      <td>-0.289103</td>\n",
              "      <td>-0.639532</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>-0.387635</td>\n",
              "      <td>-0.376387</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.110249</td>\n",
              "      <td>tcp</td>\n",
              "      <td>private</td>\n",
              "      <td>S0</td>\n",
              "      <td>-0.007762</td>\n",
              "      <td>-0.004919</td>\n",
              "      <td>-0.014089</td>\n",
              "      <td>-0.089486</td>\n",
              "      <td>-0.007736</td>\n",
              "      <td>-0.095076</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.480197</td>\n",
              "      <td>-0.289103</td>\n",
              "      <td>1.608759</td>\n",
              "      <td>1.618955</td>\n",
              "      <td>-0.387635</td>\n",
              "      <td>-0.376387</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>abnormal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.110249</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>-0.007723</td>\n",
              "      <td>-0.002891</td>\n",
              "      <td>-0.014089</td>\n",
              "      <td>-0.089486</td>\n",
              "      <td>-0.007736</td>\n",
              "      <td>-0.095076</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.383108</td>\n",
              "      <td>0.066252</td>\n",
              "      <td>-0.572083</td>\n",
              "      <td>-0.602433</td>\n",
              "      <td>-0.387635</td>\n",
              "      <td>-0.345084</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.110249</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>-0.007728</td>\n",
              "      <td>-0.004814</td>\n",
              "      <td>-0.014089</td>\n",
              "      <td>-0.089486</td>\n",
              "      <td>-0.007736</td>\n",
              "      <td>-0.095076</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.480197</td>\n",
              "      <td>-0.289103</td>\n",
              "      <td>-0.639532</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>-0.387635</td>\n",
              "      <td>-0.376387</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125968</th>\n",
              "      <td>-0.110249</td>\n",
              "      <td>tcp</td>\n",
              "      <td>private</td>\n",
              "      <td>S0</td>\n",
              "      <td>-0.007762</td>\n",
              "      <td>-0.004919</td>\n",
              "      <td>-0.014089</td>\n",
              "      <td>-0.089486</td>\n",
              "      <td>-0.007736</td>\n",
              "      <td>-0.095076</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.480197</td>\n",
              "      <td>-0.289103</td>\n",
              "      <td>1.608759</td>\n",
              "      <td>1.618955</td>\n",
              "      <td>-0.387635</td>\n",
              "      <td>-0.376387</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>abnormal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125969</th>\n",
              "      <td>-0.107178</td>\n",
              "      <td>udp</td>\n",
              "      <td>private</td>\n",
              "      <td>SF</td>\n",
              "      <td>-0.007744</td>\n",
              "      <td>-0.004883</td>\n",
              "      <td>-0.014089</td>\n",
              "      <td>-0.089486</td>\n",
              "      <td>-0.007736</td>\n",
              "      <td>-0.095076</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.447834</td>\n",
              "      <td>-0.289103</td>\n",
              "      <td>-0.639532</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>-0.387635</td>\n",
              "      <td>-0.376387</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125970</th>\n",
              "      <td>-0.110249</td>\n",
              "      <td>tcp</td>\n",
              "      <td>smtp</td>\n",
              "      <td>SF</td>\n",
              "      <td>-0.007382</td>\n",
              "      <td>-0.004823</td>\n",
              "      <td>-0.014089</td>\n",
              "      <td>-0.089486</td>\n",
              "      <td>-0.007736</td>\n",
              "      <td>-0.095076</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.480197</td>\n",
              "      <td>-0.289103</td>\n",
              "      <td>0.979238</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>-0.355014</td>\n",
              "      <td>-0.376387</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125971</th>\n",
              "      <td>-0.110249</td>\n",
              "      <td>tcp</td>\n",
              "      <td>klogin</td>\n",
              "      <td>S0</td>\n",
              "      <td>-0.007762</td>\n",
              "      <td>-0.004919</td>\n",
              "      <td>-0.014089</td>\n",
              "      <td>-0.089486</td>\n",
              "      <td>-0.007736</td>\n",
              "      <td>-0.095076</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.480197</td>\n",
              "      <td>-0.289103</td>\n",
              "      <td>1.608759</td>\n",
              "      <td>1.618955</td>\n",
              "      <td>-0.387635</td>\n",
              "      <td>-0.376387</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>abnormal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125972</th>\n",
              "      <td>-0.110249</td>\n",
              "      <td>tcp</td>\n",
              "      <td>ftp_data</td>\n",
              "      <td>SF</td>\n",
              "      <td>-0.007737</td>\n",
              "      <td>-0.004919</td>\n",
              "      <td>-0.014089</td>\n",
              "      <td>-0.089486</td>\n",
              "      <td>-0.007736</td>\n",
              "      <td>-0.095076</td>\n",
              "      <td>...</td>\n",
              "      <td>0.490690</td>\n",
              "      <td>-0.289103</td>\n",
              "      <td>-0.639532</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>-0.387635</td>\n",
              "      <td>-0.376387</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125973 rows Ã— 45 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0388b315-36ba-4da3-91f6-655356fb5e42')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0388b315-36ba-4da3-91f6-655356fb5e42 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0388b315-36ba-4da3-91f6-655356fb5e42');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pie chart distribution of normal and abnormal labels\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(bin_data.label.value_counts(),labels=bin_data.label.unique(),autopct='%0.2f%%')\n",
        "plt.title(\"Pie chart distribution of normal and abnormal labels\")\n",
        "plt.legend()\n",
        "# plt.savefig('plots/Pie_chart_binary.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "NqjG9Rorj_TZ",
        "outputId": "f8eb950c-2380-40bb-bce8-fcb68528c8db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAHRCAYAAAASbQJzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deZgcZb3+//enZ7LMZJmsBBICTQgJWSBAIOzKEtYBjwh8D8oiuyiIuEH7cys5CAMHBdGjgHjCUUBAQEBaUVCJLEIWQjYCWWACIQlZSDqZTCaz1e+PqkBnMnt65umuul/XNdf0Wn33evdTVV1lvu8jIiISRwnXAURERFxRCYqISGypBEVEJLZUgiIiElsqQRERiS2VoIiIxJZKsAPMrMrMRuV4mpVmNjWX09yVDGb2/5nZfTmc9sePmZndb2Y35XDad5vZ93M1vQ7c7pfN7MPwvg3u7tvvLDM7zsxWdNNtdfq5zof3RGeY2cVm9lIL5yXNzDez4nZMp9PPU3c+x1GhEmwifANuDT/gPgzfzH0BfN/v6/v+O64ztsbMXjCzyzt7fd/3b/Z9v83rt/d2cvWYNfcB4/v+Vb7v/9euTruDOXoAPwVODu/b+u68fRHJLZVg8870fb8vcAhwKPA9x3naZIG8eT7b8423QA0DegMLu/qGIvwY5gU9vgIqwVb5vv8B8BdgIkA4O2N0eLiXmd1uZu+FI8a7zaykpWmZ2RVmtsjMNpvZm2Z2SNbZB5nZPDPLmNkjZtY7vM5AM3vGzNaa2Ybw8J5Z03zBzH5sZi8D1cDvgGOBX4Qj2V+0kOVCM1tuZuvN7LtNzvPM7IHwcG8zeyC83EYzm2lmw8zsx83dTvj4XG1mS4AlTR+z0BAzey58HKab2d7h5XaaXbR9tGlm44C7gSPD29sYnr/DLLfwMV5qZh+Z2dNmNjzrPN/MrjKzJeF9+R8zsxYen15mdqeZrQz/7gxPGwO8HV5so5n9o5nrbr8fXwxfG+uyH+OWph2ed5yZrTCzG8xsNTAtfD7+ED4Pm81svpmNMbPvmNkaM3vfzE7Omv4lWa+zd8zsS83dxxbu98/C6W0ys9lmdmzWeZ6ZPWpmvw2nvdDMDs06/2Azez087xGCLwot3c6+ZvaP8HW1zsweNLMBTS52WPg+2WBm07LeE9sfo2+G93+VmV2SNe2yMOPa8DX+PQu/HFowN+FlM7vDzNYDXvga+qWZ/SV8bb1sZruHz8sGM3vLzA7Omn7KzJbZJ+/js9r7+DZ5DNp8nixYNLHOgrlT52ed3u7PnvC19EF4O2+b2YmdyRtpvu/rL+sPqASmhodHEnzj/6/wuA+MDg/fATwNDAL6AX8CbmlhmucCHwCHAQaMBvbOur0ZwPBwWouAq8LzBgNnA6XhbfwBeDJrui8A7wETgGKgR3ja5a3cv/FAFfApoBfBrL36rPvsAQ+Eh78U3q9SoAiYDPTPuu3Lm0zbB54L70dJM4/Z/cDmrNv+GfBSeF4yvGxxk/t3eXj44u2XzTr/fuCm8PAJwDqC0Xsv4OfAv5pkewYYAOwFrAVObeExuhF4FdgNGAq8kvUa2Clnk+tuP//XQAkwCdgGjGvHtI8Ln4tbw/tQEj4fNcAp4XP8W+Bd4Lvh830F8G7W7ZcD+xK8zj5N8OXokKzpr2jltXEBwWuuGPgmsBronfW6qAFOD18LtwCvhuf1BJYDXw8znQPUbX9umrmd0cBJ4X0cCvwLuLPJe3ABwftvEPBy1vO8/TG6Mbyt08P7ODA8/7fAUwTvlySwGLgs6zVUD3w1vI8lBK+hdQSv7d7AP8LH96Lwft4E/LPJe3k4wQDiP4EtwB4tvUabeV0Ut/N5qid4b/YKz98CjG3rsyf7OQbGAu8Dw7My7Ov6Mzbf/pwHyLe/8A1YBWwM39i/pMkHevjC3ZL9ggKOJOvDqMk0/wp8rZXbuyDr+G3A3S1c9iBgQ9bxF4Abm1zmBVovwR8AD2cd7wPU0nwJXkrwIX1gM9PZ6XbCx+eEZk7LLsHs2+4LNBB82O3wIdH0Npr7gGHHEvwNcFuTadcByawcx2Sd/yiQauExWgacnnX8FKAyPLxTzibX3X7+nlmnzQDOa8e0jwufi95Z53vAc1nHzyR4fRaFx/uFtzeghTxPbn/t0UYJNnPdDcCkrBzPZ503HtgaHv4UsBKwrPNfoYUSbOZ2PgvMafKeuCrr+OnAsqz7sLXJ62QNcARBadUC47PO+xLwQtZr6L1mXkO/zjr+VWBR1vEDgI2tZH8D+I+WXqPNvC5aet00fZ7qgT5NXq/fp43PHnYswdHhYzMV6NHe5z1uf5on3rzP+r7/fCvnDyUYHc3OmqNmBG/C5owk+PBryeqsw9UE3zQxs1KCb32nAgPD8/uZWZHv+w3h8fdbmW5zhmdfx/f9LeGsoeb8Lsz+cDi76gHgu77v17Uy/bbyZN92lZl9FGb6sD3hWzEceL3JtNcDIwg+VGHnx7lvK9NannV8eXhaR7R0W21Ne63v+zVNppX92GwF1mU9/1vD/30JZtGeBvwQGEMwWikF5rcnsJl9C7gszOMD/YEhrdyn3hbMvh4OfOCHn7xZ96ul2xlGMBfgWIISTxAUbrbs11HTx2i97/v1TbL0DbP2YOfHd0QL092u6ePb9PjHrxMzuwj4BkGpkXW7HdKO52mD7/tbso5vfwza/dnj+/5SM7uO4AvMBDP7K/AN3/dXdjRvlGmZYOesI3hzTPB9f0D4V+YHK9M0532CWR8d9U2CWRqH+77fn+AbNwQv+u38JtdperypVQTFFkwoKNpmV/P3fb/O9/0f+b4/HjgKOINgNlFrt9PW7Wffdl+CWTorCb7dQvAG3273Dkx3JbB31rT7ENyvD9q4XpvTIph9mqsPjram3db9bFG4bPFx4HZgmO/7A4A/s+PrpaXrHgtcD/w/glmLA4BMe65L8JoaYbbDMta9Wrn8zQT384DwdX1BM7czMutwex//dQSj/6aPb/ZrYFce370JZnNfAwwOH6MFtO8xyp5Oe56ngeFreLvtj0GHPnt833/I9/1jCB4Tn2BWu2RRCXaC7/uNBG+GO8xsNwAzG2Fmp7RwlfuAb5nZZAuMDt9QbelH8ILfaGaDCL45tuVDoLXfMj4GnGFmx5hZT4JlK82+DszseDM7wMyKgE0EHzCN7bydlpyeddv/RbBc6X3f99cSfFhdYGZFZnYpO35x+BDYM7xec34PXGJmB4UfMjcDr/m+X9mJjL8HvmdmQ81sCMEs5Ac6MZ3unnZPgmVIa4H6cLRxcutX+Vg/gllwa4FiM/sBwUiwPf4dXvdaM+thZp8DprRxW1VAxsxGAN9u5jJXm9me4ev+u8AjbYUIR8ePAj82s37he+wb5O7x7UNQJGshWLmFcKW5Dmrv8/QjM+sZfkE5A/hDRz57zGysmZ0Qvh9qCD5LGpteLu5Ugp13A7AUeNXMNgHPE4zaduL7/h+AHwMPEawY8iTBCKgtdxIsvF9HsDLFs+24zs+Ac8I12+5qJstC4OowyyqC2VAt/bh2d4LS3ESwws50glmkbd5OKx4iKPOPCFZGuCDrvCsIPhDXE6zs80rWef8gWElptZmta+Z+PU+wzOTx8H7tC5zXgVzZbgJmAfMIZlG9Hp6WC102bd/3NwPXEhTBBuALBCtQtMdfCV5fiwlmvdXQzlntvu/XAp8jWCb2EcEKI0+0cpUfEazAlAHSLVz2IeBvwDsEixLa+xh9lWCuwjvAS+F0/red122V7/tvAj8hKP0PCZYXvtyJ6bTneVodnrcSeJBgGelb4Xnt/ezpBVQQfH6sJlgZ6zsdzRt1tuNsfBERkfjQSFBERGJLJSgiIrGlEhQRkdhSCYqISGypBEVEJLZUgiIiElsqQRERiS2VoIiIxJZKUEREYkslKCIisaVdKYmIODZ79uzdiouL7yPYILcGJ53TCCyor6+/fPLkyWvaeyWVoIiIY8XFxfftvvvu44YOHbohkUhog86d0NjYaGvXrh2/evXq+4DPtPd6+sYhIuLexKFDh25SAXZeIpHwhw4dmqGDu7dSCYqIuJdQAe668DHsUK+pBEVExLkRI0YcsGrVqm5fRKdlgiIieSaZSk/O5fQqK8pn53J6TdXV1dGjR4+uvIkuo5GgiIjw9ttv9xw1atSE8847b+/Ro0dPOProo/erqqqyV155pWTSpEn7jxkzZvxJJ52079q1a4sApkyZMvbSSy8dOXHixHE33XTTsClTpoy97LLLRk6cOHHcqFGjJkyfPr305JNP3nfvvfeeeO211w7ffjtTp07dd8KECeNGjx494fbbbx/i7h4HVIIiIgLAe++91/vaa69ds3Tp0oVlZWUNv/3tbwdefPHF+9x8880rFi9e/OaECRO23nDDDR8XWm1trS1YsGDRj370ow8Bevbs2bhgwYJFl1xyydpzzz139K9//ev33nrrrYWPPPLIkNWrVxcBPPjgg5ULFy5c9MYbb7x5zz33DNt+uisqQRERAWDEiBHbjjrqqK0ABx98cPWyZct6bd68uai8vLwK4Iorrlj/6quv9t1++c9//vMfZV//rLPO2ggwadKkraNHj966995715WUlPgjR47c9s477/QEuPXWW4eNHTt2/OTJk8etXr26x8KFC3t33z3cmZYJiogIAD179vx4DdWioiJ/48aNrS7o69evX2P28d69e/sAiUSCXr16fTytRCJBfX29PfPMM/2mT5/eb9asWW/169evccqUKWO3bt3qdDCmkaCIiDSrrKysoX///g3PPvtsX4Df/OY3g4888siqzk5v48aNRWVlZQ39+vVrnDNnTu+5c+f2yV3azlEJiohIi6ZNm/buDTfcsOeYMWPGz5s3r6SiomJlZ6d19tlnZ+rr623UqFETvv3tb4+YNGnSllxm7Qzzff0+U0TEpblz51ZOmjRpnescUTB37twhkyZNSrb38hoJiohIbKkERUQktlSCIiISWypBERGJLZWgSDuYWaWZOd/Ek4jklkpQIs/MtFEIEWmWSlAKgpklzWyRmf3azBaa2d/MrMTMDjKzV81snpn90cwGhpd/wczuNLNZwNfC43eY2axwOoeZ2RNmtsTMbsq6nSfNbHZ4G1c6u8MieaC0tPRg1xma841vfGP4D37wg2G5mJa+IUsh2Q/4vO/7V5jZo8DZwPXAV33fn25mNwI/BK4LL9/T9/1DAczsTKDW9/1DzexrwFPAZOAjYJmZ3eH7/nrgUt/3PzKzEmCmmT0eni7SfbyynO5KCS/TpbtS6oh82+2SSlAKybu+778RHp4N7AsM8H1/enja/wF/yLr8I02u/3T4fz6w0Pf9VQBm9g4wElgPXGtmZ4WXG0lQvCpBibypU6fuu2rVqp7btm1LXHXVVR9+61vfWgdw2WWXjZw+fXr/oUOH1j3++OPvDB8+vH7KlCljJ0+eXPXSSy/137x5c9Hdd99deeqpp1ZVV1fbRRddtPe8efNKi4qKuO22294/88wzN991112Dn3zyyYHV1dWJhoYGu/DCC9c9/fTTA6qrqxPLly/vffXVV6+ura1NPPLII4N79uzZ+Le//W3JsGHDGn7yk58MmTZt2tC6ujpLJpPbHnvssXebbq90V2l2qBSSbVmHG4ABbVy+6SaZtl+/scm0GoFiMzsOmAoc6fv+JGAO4HQL9yLdpbldHG3dujVx6KGHblm6dOnCo48+enMqlfp4N0r19fU2f/78Rbfeeuv7N95443CAW2+9dTczY/HixW8+9NBD71x55ZXJ6upqA1i4cGHpU089tWzmzJlvAyxevLgknU4vmzlz5qJbbrllRGlpaeOiRYvePPTQQ7fcc889gwHOP//8DQsWLFj09ttvvzl27Nitd911V85XTlMJSiHLABvM7Njw+IXA9FYu35YyYIPv+9Vmtj9wxK4GFCkUze3iKJFIcPnll38EcOmll66fMWPGx7tROvfcczcAHHXUUVtWrFjRE+CVV17pe+GFF64HOPjgg2uGDx9eO3/+/N4Axx577KZhw4Y1bL/+UUcdtXngwIGNw4cPr+/bt2/DueeeuxHggAMOqK6srOwFMHv27JLJkyePHTNmzPjHH398cFfsdkmzQ6XQfRG428xKgXeAS3ZhWs8CV5nZIuBt4NUc5BPJe+3dxZGZfXx4+26TiouLaWhosKaXbaq0tHSH2ZjZu21KJBI77Iapvr7eAK688sp9HnvssaVHHnnk1rvuumvw9OnT+3X6TrZAI0EpCL7vV/q+PzHr+O2+73u+77/h+/4Rvu8f6Pv+Z33f3xCef5zv+7OyLv/xcd/3X/B9/4ym5/m+v833/dN83x8XTus43/dfCC+T9H1fGziWSGppF0eNjY1MmzZtIMD9998/eMqUKZtbm87RRx9d9cADDwwCmDdvXq9Vq1b1PPDAA2s6m6u6ujqx11571W3bts0efvjhQZ2dTms0EhQRibmzzz47c++99w4dNWrUhFGjRtVs38VRSUlJ44wZM/r893//9/DBgwfXPfHEE++0Np3rr79+zUUXXbT3mDFjxhcVFXHPPfdUlpSUdHpXRalUauWUKVPGDRo0qP6QQw6pqqqqKurstFqiXSmJiDimXSnljnalJCIi0k4qQRERiS2VoIiIxJZKUETEvcbGxsY2f2YgrQsfww5tUUYlKCLi3oK1a9eWqQg7r7Gx0dauXVsGLOjI9fQTCRERx+rr6y9fvXr1fatXr56IBied1QgsqK+vv7wjV9JPJEREJLb0jUNERGJLJSgiIrGlEhQRkdjSijEiOZBMpXsT7Iqpf9b//k1O6wMUAZb1R5PjPrAV2AxUtfB/E7CmsqK8uhvumkikacUYkTYkU+kBwN7AXuHfSGBPYDiwR/i/v4NoGWBlC38fAMsqK8rXOMglUjBUgiKhZCo9HBgHjA//th8e6jLXLtpAsG/Et8K/7YeXVVaU17kMJpIPVIISO8lUuhg4gGDP8ZP5pPAGuMzVzeqBZcAcYAYwE3hds1glblSCEnnJVHoEQeEdARxOUHylTkPlpwZgIZ+U4gxgQWVFeb3TVCJdSCUokZNMpUcDJwEnAEcCI9wmKmjVwMvA88DfgTmVFeUd2jajSD5TCUrBS6bSZcCJwMkE5TfKbaJIWw/8k6AQn6+sKF/qOI/ILlEJSsFJptJGMFvzNILiO4zgpwfS/ZYDzwFPA89VVpTXOM4j0iEqQSkIyVQ6ARwDnAOcTfCzBMkvW4C/An8EnqmsKN/oOI9Im1SCkreSqXQR8CmC4vscsLvbRNIBtQTLEf8APFVZUb7BcR6RZqkEJe8kU+lPAV8AzgJ2cxxHdl0dwQjxfuBp/T5R8olKUPJCMpXeHfgicCkwxnEc6TrrgAeAaZUV5fNchxFRCYoz4ezO04HLgHK0Ldu4mQ1MAx7S7FJxRSUo3S6ZSo8CLicY+WkFF9lGsDLNLyoryl92HUbiRSUo3SaZSh8LfAs4k0/2oCCSbSZwB/AHbalGuoNKULpUOMvzHOCbBL/nE2mP94FfAPfqpxbSlVSC0iWSqXRfglmeXwOSbtNIAdtCsFbpzyorypc4ziIRpBKUnEqm0sOArwNfIl57ZZCu1Qg8AXiVFeULXYeR6FAJSk4kU+lBwA3ANWgPDdJ1Ggl+gP+jyoryRa7DSOFTCcouCTde/Q3gOtzsXV3iqRF4mKAMF7sOI4VLJSidkkyl+wDXEqztOchxHImvBuAh4Ebt0UI6QyUoHZJMpXsBXwFSaJNmkj/qCX54/73KivI1rsNI4VAJSrslU+lzgdvQ2p6SvzYBNxGsTVrrOozkP5WgtCmZSh8C3Akc6zqLSDstBb5VWVH+lOsgkt9UgtKiZCo9BLiFYKPWCcdxRDrjeeDrlRXlC1wHkfykEpSdhDuwvYpgttJAx3FEdlUDcC/B8sKPXIeR/KISlB0kU+lDCT4wDnadRSTH1gBfq6wof9h1EMkfKkEBIJlK9wZuJPjNX5HjOCJd6U/Alysryj9wHUTcUwkKyVT6aOB/0c5sJT42EWzh6J7KinJ9CMaYSjDGkql0KcGKL9egFV8knv4FXK6Nc8eXSjCmkqn08cB9wCjXWUQcqwE84PbKivIGx1mkm6kEYyaZSpcAPyXYy4N2bCvyiReB8ysryt93HUS6j0owRpKp9ETgEWC86ywieWoDcEVlRfnjroNI91AJxkQylb6KYARY4jqLSAG4l+BH9tWug0jXUglGXLiro/uAc1xnESkwi4DzKivK57kOIl1HawRGWDKVPgJ4AxWgSGeMA2YkU+lrXQeRrqORYAQlU2kDrifY7Fmx4zgiUfAE8MXKivIq10Ekt1SCEZNMpfsCvwM+6zqLSMQsBP6jsqJ8mesgkjsqwQhJptL7AE8BB7jOIhJRG4DPV1aU/9V1EMkNLROMiGQqfRwwExWgSFcaCPw5mUpf7zqI5IZGghGQTKW/AvwMLf8T6U4PA5fpZxSFTSVYwJKpdA/g5wRbfxGR7vcG8NnKivLlroNI56gEC1QylR5MsMbap1xnEYm51cAZlRXls10HkY7TMsEClEyl9wJeQgUokg92B15IptKnuQ4iHacSLDDJVHo88DKwv+ssIvKxvsDTyVT6MtdBpGNUggUkmUofSbCl+z1dZxGRnRQD9yVT6e+5DiLtp2WCBSKZSpcDjwKlrrOISJvuAq7TXuvzn0qwACRT6S8SbARbP4EQKRy/J9jUWp3rINIyzQ7Nc8lU+lvANFSAIoXm88BjyVS6p+sg0jKVYB5LptLfAf4b7QFepFB9BhVhXtPs0DwVbpbpVtc5RCQnngHOrqwor3UdRHakkWAeCmeBqgBFouMM4HGNCPOPRoJ5JplKfx34qescItIl0gQjwm2ug0hAJZhHkqn014A7XecQkS71Z+BzKsL8oBLME8lU+hqCjWGLSPSlCTa8Xe86SNxpmWAeCDe1pAIUiY9y4DfJVFprfjumEnQs3BLMPa5ziEi3uwiocB0i7jQ71KFkKj0F+CfaFJpInH29sqJc6wI4ohJ0JJlK70ewN4ihrrOIiFM+cH5lRfnvXQeJI5WgA8lUejfg38Ao11lEJC/UEuyY9znXQeJGJdjNkql0X+AFYLLjKCKSX6qA47SH+u6lFWO6UTKVLgb+gApQRHbWF0gnU+mRroPEiUqwe90FnOo6hIjkrWHAH5OpdInrIHGhEuwm4W8Bv+w6h4jkvckE+w+VbqBlgt0gmUofDkwHernOIiIF44bKivLbXIeIOpVgF0um0rsDs4HhrrOISEFpBMorK8qfdR0kylSCXSiZSvcg+DH80a6zSPus+NWlJHqWQCKBJYrY44t3svFfv6N66WtgRlHpAAaffh3F/QY3e/3GbdWsvO/LlI45gkEnBXO//YY6Pnrubmremw+WYMCnLqTP2KPZNPtPVL3xF4r6D2W3z30PK+pBzYqFVL/9CoNOvKI777bkr43AlMqK8iWug0RVsesAEXcXKsCCM+zzN1NUWvbx8f6Hn82AT10IwKZZT5N55fcMPuWaZq+78cXf0WvkxB1Oy7zyKInSAYy48l58v5HGrZsB2LLwBfa49Bdk/v0oW999nZJ9p5B5+WGGfOb6LrpnUoAGAE8lU+kjKivKN7kOE0VaMaaLJFPpy4GrXOeQXZfo9clW7fy6GqD5bR5vW72Uhi0bKdnn4B1Or5r/HGVHnAuAWSKrYH1oaMCv24Ylitmy8J+UjDqUopJ+XXE3pHCNA+53HSKqVIJdIJlKTwb+x3UO6QQz1jz6A1bd/zU2v/HJopgN//otK355MVvefIEBx16w09V8v5EN/7iPgcdftsPpjTVVQDBCXHX/11j75C00bNkAQL9DzmDV775Jw6a19Boxjqr5z9PvkPIuvHNSwM5KptJfcR0iirRMMMeSqXQ/4HVgtOss0nH1m9dR3G8IDVs28uEj32PQSVfRO2v2Zubfj+LX1zHg2PN3uN6m2X/Cr99G2eHnUDX/eWpXL2HQSV+moTrDip+fz5D/SNFn/2PYNOOP1K55hyFnfHOH6298+ff0HJoES7Blwd8p6j+UgSdchpm+p8rHagiWD853HSRK9A7LvV+iAixYxf2GAFDUZwClY45k28rFO5zfZ8JxVC9+eafrbVv5Fptnp1nxq0vZ8M//pWrBP9jwwv0kSvpjPXpROvYoAEr3P4ba1ct2uG795vXUrlpM6Zgj2TTzjwz5jxtI9OpDTeXcLrqXUqB6Aw8nU2ntdSaHtGJMDiVT6YuAneeVSUForK0Bv5FEr1Iaa2uoeXcOZUd/nrqPPqDHoBEAVC95jR6D9tzpukPP/PbHh7ePBAcedzEAJftOoea9+ZTsPYma5XPpMWTHrWJtfPEByo4JRpZ+/TYwA7PgsMiOxgN3Ale6DhIVKsEcSabSo4BfuM4hnddQvZG1T9wUHGlspM/4T1MyajJr/3gzdR+tAEtQ3H8og065GoBtq5ZQ9cZfGHzata1Od+Bxl7DumZ+w4e+/pqi0P4NPv+7j82o/DEaFvXYPZh70GXccq35zDUX9h1B2+DldcC8lAq5IptJ/q6wof8x1kCjQMsEcSKbSRcCLwJGus4hILGwEDqqsKF/uOkih0zLB3PguKkAR6T4DgIeSqbQ+w3eRHsBdlEylpwDfd51DRGLnKOC6Ni8lrdLs0F0QbhZtDjDBdRYRiaWtwAGVFeXL2rykNEsjwV3zHVSAIuJOCXBfMpVufjNG0iaVYCclU+lxBMsCRURcOg7QFtc7SbNDOyH81vUi2ji2iOSHTcCEyoryFa6DFBqNBDvny6gARSR/9Afudh2iEGkk2EHJVHpPYCHBi05EJJ9cUFlR/qDrEIVEI8GO+yUqQBHJT3cmU+mBrkMUEpVgByRT6bOAM13nEBFpwRDgR65DFBLNDm2nZCrdC3gTGOU6i4hIK+qBSZUV5W+6DlIINBJsv+tQAYpI/isG7nAdolBoJNgOyVR6N2Ap0M91FhGRdvqPyoryp12HyHcaCbbPTagARaSw/DSZSvd0HSLfqQTbkEylDwQuc51DRKSD9gW+7jpEvlMJtu0O9DiJSGH6bjKV3t11iHymD/dWJFPpzwAnuM4hItJJ/dBPJlqlFWNaEO4tfiEw1nUWEc6vsn0AABvgSURBVJFdUAeMrawof9d1kHykkWDLvoAKUEQKXw/gB65D5CuNBJsRjgIXAfu5ziIikgMNwPjKivLFroPkG40Em3c+KkARiY4i4IeuQ+QjjQSbCEeBbwGjXWcREcmhRuAAbU5tRxoJ7uwCVIAiEj0JwHMdIt9oJJglmUoXE4wC93WdRUSkC/jAwZUV5XNdB8kXGgnu6AJUgCISXYbWFN2BRoKhZCqdAN5Gs0JFJNoagf0rK8qXuA6SDzQS/MRnUAGKSPQlgG+4DpEvVIKfuM51ABGRbnJxMpUe6jpEPlAJAslU+iDg065ziIh0k97A1a5D5AOVYECjQBGJmy8nU+lerkO4FvsSTKbSw4DzXOcQEelmuwGfdx3CtdiXIPBlIPbfhkQklr7mOoBrsf6JRDgrYDkwzHUWERFHPl1ZUf4v1yFciftI8DxUgCISb1e4DuBS3EvwS64DiIg4dnYylS5zHcKV2JZgMpUeAxzpOoeIiGMlxHgFmdiWIHCx6wAiInniMtcBXInlijHhdkLfA0a4ziIikicOrKwon+86RHeL60jwJFSAIiLZLnUdwIW4luDFrgOIiOSZC5KpdE/XIbpb7EowmUoPAD7rOoeISJ4ZQrA3nViJXQkS/Dawt+sQIiJ56CLXAbpbHEvwQtcBRETy1MnJVLqf6xDdKVYlmEyl90C/DRQRaUkv4AzXIbpTrEqQYFmguQ4hIpLHznYdoDvFrQTPch1ARCTPnZZMpUtdh+gusSnBZCo9EDjOdQ4RkTxXCpzmOkR3iU0JEszn7uE6hIhIAYjNLNE4laBmhYqItM8Z4f5WIy8WJRjO3z7FdQ4RkQLRDzjZdYjuEIsSJCjA2CzoFRHJgdNdB+gOcSnBctcBREQKzEmuA3SHuJTgia4DiIgUmH2TqfQ+rkN0tciXYPgkJl3nEBEpQFNdB+hqkS9BNAoUEemsyM8SVQmKiEhLTkym0pHuiUjfudDxrgOIiBSoQcAhrkN0pUiXYDKVnggMc51DRKSARXq5YKRLEM0KFRHZVSrBAnaC6wAiIgXu8CgvF4zsHQsd7TqAiEiB6wtMcB2iq0S2BJOp9GhgsOscIiIRMMV1gK4S2RIEDncdQEQkIiL7eaoSFBGRtmgkWIAi+6SJiHSzieEu6SInkiWYTKWLgUmuc4iIREQRMNl1iK4QyRIExgG9XYcQEYmQSC5iimoJRnozPyIiDhzmOkBXUAmKiEh7RPK3glEtwUg+WSIiDu2XTKWLXIfItaiW4FjXAUREIqYnMMp1iFyLXAkmU+k+wAjXOUREImic6wC5FrkSBPYDzHUIEZEI2t91gFyLYglqVqiISNfQSLAAqARFRLqGSrAAjHEdQEQkojQ7tABoJCgi0jXKkqn07q5D5FIUS1AjQRGRrjPSdYBcilQJJlPpAUB/1zlERCIsUj9Bi1QJAnu4DiAiEnEqwTymEhQR6VoqwTymEhQR6VoqwTymEhQR6VoqwTymEhQR6VoqwTymEhQR6VoqwTymEhQR6Vr9kql0X9chckUlKCIiHTXQdYBciVoJDnIdQEQkBvq5DpArUSvByAzRRUTyWGS2zBWZEkym0gmgxHUOEZEY0EgwD/VxHUBEJCY0EsxDKkERke6hkWAe0vJAEZHuoZFgHtJIUESke2gkmIc0EhQR6R4qwTykEhQR6R49XQfIlSiVYA/XAUREYiIy3RGZOwL4rgOIiMREkesAuRKlEmx0HUBEJCYi0x3FrgPkkEpQCoTvT7DKZacUzVx9bGJ+Yx9qovQ+lBhY6w9YB+WuY+RElN58KkHJS2VUbTw+8cbSU4pmbjk0sbjvEDKjzRgNjHadTaQzxvDB664z5IpKUCSHEjQ2HGjvLDulaObqTyfmFu1rK4f3pD5pxqGus4nkUL3rALmiEhTZBYPIrD+xaM47JydmbTkksWTAIDaPNmMMMMZ1NpEupBLMQ1o7VLpUEQ31B9vSJacUzVz7qcS8on1s1cie1rAXMNh1NpFuphLMQxoJSk7txoa1Jxa9/s7JiVk1ByWWDRhA1X5mjAPGuc4m4phKMA9tcx1AClcx9XWTbfGSU4tmrjs2Mb94b/twrx7WsCcw1HU2kTykEsxDGdcBpHDswfrVU4tmV56UmL1tUmLZwP5UjzFjvOtcIgVCJZiHVILSrJ7UbZuSeGvJKYmZ649JLOg50tbsXWyNw4HdXWcTKVA1rgPkikpQImekrfngpMTs96YmZtcekHh3SF+27mfGRNe5RCJkjesAuWK+H52VKpOpdC3akHas9KK25sjEm4tPTczccFRiYa8RtjZZZL5GeCJd62S8zHOuQ+RClEaCEIwGh7gOIV1nH1v1/kmJ2e9NLZrdMN6WD+lDzX5mHOg6l0jMfOg6QK6oBCVvlVKz5ajEgqWnFs3acETizdI9WL9PkfkjgZGus4nE3GrXAXIliiUoBWq0rVh+cmL2ihOLXm8YZ+8NK2HbaDMmuc4lIjtoANa5DpErUSvBja4DSPv0YevmYxPzl55SNDNzRGJRn2FsGJUwf29gb9fZRKRVa/Eykdk4SdRKcL3rANIc39/f3n/35MTMlScUveGPtfd3703tvmYc7DqZiHRYZGaFQvRKcIXrAAL92JL5dGLe0lOLZlYdlnirz1A2jk4Yo4BRrrOJyC5TCeax910HiBujsXGCVb5zStGsVccn3rDR9sEevagbZcZk19lEpEtEZs1QUAlKBw1g84bjE28sO6VoVtXkxOL+2kGsSOxoJJjHNDs0h8IdxC49pWjmmuMSc21fWzmih3YQKxJ3KsE8ppHgLmiyg9iyQWzez4yxwFjX2UQkb2h2aB5bRbB186jdr5zTDmJFpJMiNRKM1LZDAZKp9HJgL9c58k0LO4gtdZ1LRArOeLzMItchciWKI6b3iXkJFlNfd2ji7SWnJGatPTYxr8fetkY7iBWRXNHs0DxXCRztOkR3Gs66VVOLXl9+UmL2tgMTywb1p3o/7SBWRLpAFV7mI9chcimKJRiZYXpzwh3ELj41MeOjoxMLeo60tclia9wD2MN1NhGJvHmuA+RaFEtwoesAudTCDmIPcJ1LRGLpDdcBci2KJfim6wCd1ZttW49ILFrSZAexI4ARrrOJiABzXQfItSiW4DJgG9DLdZC2aAexIlJgIjcSjNxPJACSqfRcyK8yaWEHsVpbU0QKRQPQDy+z1XWQXIriSBCC5YJOS3A/W1F5cmLWBycUzdEOYkUkCpZErQAh2iXYbYIdxM5bcmrRzM2HJ94qHcaGfRPmJ4Fkd+YQEelCkVseCNEtwS5cOcb3x9l7756cmLXyhKI5/hhbsX0HsYd03W2KiDinEiwg83M1oU92EDtj82GJt/sNZeO+2kGsiMRQ5FaKgYiuGAOQTKU/AgZ25Dqt7CDWuiimiEihGIGXWek6RK5FdSQIMBM4ubULaAexIiLtsjaKBQjRLsEZZJWgdhArItJpkVweCBEuwd1Z//KxRfNnnJKYtfXgxJL+2kGsiEinRXJ5IES4BF/t/dWZwGGg5XkiIrsosiPBhOsAXcbLrAfech1DRCQCIjsSjG4JBl50HUBEpMCto4B3TNAWlaCIiLTmb3iZRtchukrUS/Al1wFERArcs64DdKVol6CXqQRWuI4hIlKgfOCvrkN0pWiXYECzREVEOmcOXmaN6xBdKQ4l+C/XAUREClSkZ4VCPErwGYIhvYiIdMxfXAfoatEvQS+zApjlOoaISIHZCPzbdYiuFv0SDDzhOoCISIH5O16mwXWIrhaXEnzcdQARkQIT+eWBEJcS9DJLgIWuY4iIFBCVYMT80XUAEZECsSBcnyLy4lSCWi4oItI+sRgFQpxK0MvMAd51HUNEpACoBCNKs0RFRFq3hRhtaStuJahZoiIirXsGL1PrOkR3iVsJ/htY7TqEiEgem+Y6QHeKVwkG+8R6ynUMEZE89T7wnOsQ3SleJRjQLFERkeb9X5R3oNucOJbgP4ENrkOIiOQZn5jNCoU4lqCXqQP+5DqGiEiemY6Xecd1iO4WvxIMxO7bjohIG/7XdQAX4lmCXuYFYL7rGCIieWIT8JjrEC7EswQDv3AdQEQkTzyMl9nqOoQLcS7BBwh2GikiEnexnBUKcS5BL1NNjJ94EZHQQrzMa65DuBLfEgz8EojVb2JERJqI9YqC8S5BL7MM+IvrGCIijtQDv3MdwqV4l2BAK8iISFyl8TJrXIdwSSUIfwUWuw4hIuLAb1wHcE0l6GV8gmWDIiJx8haQdh3CNZVgYBpQ5TqEiEg3ujluG8tujkoQwMtsIuYLh0UkVpYCD7kOkQ9Ugp/QCjIiEhc342UaXIfIByrB7bzMm8A/XMcQEeli76I5Xx9TCe7oDtcBRES6WAVept51iHyhEszmZZ4BXnEdQ0Ski7wH3O86RD5RCe4s5TqAiEgXuRUvU+s6RD5RCTblZV5Ev50RkehZiX4cvxOVYPO+gzasLSLRchteZpvrEPlGJdgcLzMfeNB1DBGRHFkN3Os6RD5SCbbs+4C+NYlIFNwe1z3Ht0Ul2BIvsxz4lesYIiK7aC1wt+sQ+Uol2LofA5tchxAR2QU/xctscR0iX6kEW+Nl1gG3u44hItJJ7wM/dx0in6kE2/ZT4EPXIUREOuE6jQJbpxJsS/ACutF1DBGRDvozXuYJ1yHynUqwfX4NLHMdQkSknbYCX3UdohCoBNvDy9QB33MdQ0SknW7By7zjOkQhUAm23yPAa65DiIi0YTFwm+sQhUIl2F5exgcuA7TxWRHJZ9do82jtpxLsCC+zEK0kIyL561G8zHOuQxQSlWDH3Qq87jqEiEgTm4Gvuw5RaFSCHRXskflSoM51FBGRLD/Ey6x0HaLQqAQ7w8vMBW5xHUNEJDQXuMt1iEKkEuy8m4D5rkOISOz5wFfwMg2ugxQilWBnBb8dvASodx1FRGJtGl7mFdchCpVKcFd4mdloA9si4s5a4HrXIQqZSnDXecAi1yFEJHZ84CK8zHrXQQqZSnBXBT9KvRRodB1F8ltDo8/B91RxxkPVAPi+z3f/XsOYn1cx7n+quOu15n/fXHTjJg66u4qD7q7iM7+v3un8a/9SQ9+bP9nt5c9fq2XiL6s4/cFqaht8AF56r56vP1vTBfdKHPpvvMyzrkMUumLXASLBy7yKV3YH8E3XUSR//ey1WsYNSbAp7Lr736jj/U0+b13Th4QZa7Y0/z2qpBjeuKpvs+fNWtnAhhp/h9MenF/HvC/34eYXa/nr0nrOGFPMf/1rG78/uzSn90ec+jfwXdchokAjwdz5PrDEdQjJTys2NZJeUs/lh/T8+LRfzarlB5/uRcIMgN36dOzt2NDo8+3narhtaq8dTvfxqWuA6jqfHkXGA/PqOG10MYNKbNfviOSDDcB54W+WZRepBHPFy2wlWFtUqynLTq57tobbpvYmkdVDyzb4PLKgjkPvreK0B7ewZH3zL52aejj03iqOuG8LT771yTYafjGjls+MKWaPfju+ja85rCdH/GYL72V8jh5ZxLQ36rj6sJ5NJyuF6xK8zHuuQ0SFSjCXvMzLwA2uY0h+eWZxHbv1MSYPL9rh9G31Pr2LYdaVfbnikJ5c+nTzy+yWX9eXWVf25aGzS7ju2RqWfdTIys2N/OHNer56+M7lduGknsz5Ul8e+FwJd7xay7WH9+QvS+s559Fqvv5sDY2+38ytSIH4GV7mKdchokQlmGte5ifA713HkPzx8nsNPP12Pck7N3PeY1v5x7v1XPDEVvbsn+Bz43oAcNb+xcz7sPmR4Ij+wdt01MAExyWLmbO6gTmrGlj6USOj76oieedmqutg9F2bd7jeys2NzPiggc/u34Of/LuWR84pYUBv4+/vaGZFgZqFfg6Rc1oxpmtcDowHJrkOIu7dMrU3t0ztDcALlfXc/kotD3yuhNTzNfyzsp59BvZk+vIGxgze+Tvphq0+pT2gV7GxrrqRl99v4PqjezJ+aBGrv9Xj48v1vXkTS6/tt8N1v/+Pbdx4fLC8cGudjxkkLFhWKAVnE/CfeBntyi3HVIJdwctU45V9luCb22DXcSQ/pY7pxflPbOWOV2vp29O478wSIFjj8+5Ztdz3mRIWrWvgS8/UkDBo9CEVFmBb5qwKRnuH7BFc9gsH9OCAX21hZH/j+qO1lmgBulx7iu8a5mv5QNfxyqYCzwJtf2qJiDTvHrzMVa5DRJWWCXYlL/M88B3XMUSkYM0FrnMdIso0EuwOXtnDwH+6jiEiBaUKOBQv87brIFGmkWD3uBSY5zqEiBSUK1WAXU8l2B28TDVwFvCR6ygiUhB+gJfRT626gWaHdiev7GTgL+jLh4i0bBpe5lLXIeJCH8bdycv8DW30VkRa9hxwpesQcaKRoAte2aPAua5jiEhemQ8cg5fZ1OYlJWc0EnTji8B01yFEJG+sBMpVgN1PJehCsMeJM4BXXUcREec2AWfgZd53HSSOVIKueJkq4DRgjusoIuJMDfAZvIw+BxxRCbrkZTYCJwMLXUcRkW5XT7BRbC0acUgl6JqXWQdMRXulF4kTH7gML/O06yBxpxLMB15mNXAisNx1FBHpFt/Ey/zWdQhRCeaPYKH4CQRriYlIdP0YL3OH6xAS0O8E841Xtj/Bzyd2cx1FRHLuZ3gZ7RUij2gkmG+8zFvAScAG11FEJKd+qALMPxoJ5iuv7DDgeaC/6ygiskt84Fq8zC9cB5GdqQTzmVd2DPBXoNR1FBHplHrgYrzMg66DSPM0OzSfeZmXgDOBza6jiEiHbQXOUgHmN40EC4FXdiCQBvZ0HUVE2iUDnImXedF1EGmdSrBQeGUjCIpwkusoItKqNcCp2hRaYdDs0ELhZT4AjgWedR1FRFq0nGB3SCrAAqESLCReZjPBMsJ7XUcRkZ0sIihAbQKxgGh2aKHyym4AbgHMdRQRYSZwGl5mvesg0jEqwULmlf0/4P+A3q6jiMTY34HPhrtHkwKj2aGFzMs8SrAHCn37FHFjGsEe4VWABUojwSjwyvYD/gyMdh1FJCZqgGvwMr9xHUR2jUowKryyIcBTwFGuo4hE3DvAOVoDNBo0OzQqgp3zngg86jqKSIQ9DUxWAUaHRoJR5JV9A6gAeriOIhIRDcB3gdvwMvrQjBCVYFQFe6F4BNjHdRSRAvchcB5e5gXXQST3NDs0qrzMTOBg4DHXUUQK2IvAwSrA6NJIMA68sq8APwV6uY4iUkB+AqTwMvWug0jXUQnGhVc2iWD26FjXUUTy3CaCfQD+0XUQ6XqaHRoXXmYuwezR/yHY07WI7GwecKgKMD40Eowjr+xkgi1dDHcdRSRP1BKsUX0zXmab6zDSfVSCceWVDQJ+Bfw/11FEHHsJuBIvs8h1EOl+KsG488q+QDCLdIDrKCLdLAPcANyr3/7Fl0pQwCvbk2Dt0XNdRxHpJo8DX8XLrHIdRNxSCconvLLjgbuAia6jiHSRFcDVeJmnXQeR/KC1Q+UTXuafwEHAtcAGx2lEcqkR+DkwXgUo2TQSlOYFe6X4MXA5+rIkhW0+cAVe5jXXQST/qASldV7ZwQTfoI92HUWkg2qAG4Hb8TJ1rsNIflIJSvt4ZecDt6HfFkr+84E/AjfgZZa6DiP5TSUo7eeV9QW+B3wd6Ok4jUhz0sD3tb8/aS+VoHScVzYauBModx1FJPQc8AO8zKuug0hhUQlK53llpwDfR8sLxZ1/EYz8/uU6iBQmlaDsOq/sGOA7wOmuo0hsvEZQfs+5DiKFTSUoueOVHUiwGar/BIocp5FomkMw2/MZ10EkGlSCknte2T7At4FLgN6O00g0LAB+CPxR2/mUXFIJStfxynYDrgO+ApQ5TiOFaS5wK/AIXqbRdRiJHpWgdD2vrD/wZYJC3N1xGsl/1cAjwN14mRmuw0i0qQSl+3hlvYCLCWaV7us2jOShBcA9wO/wMhnXYSQeVILS/byyIuAk4ALgLKDUbSBxqAb4A8Go7xXXYSR+VILiVrAVms8BFwInoI11x8VbBKO+3+JlPnIdRuJLJSj5wysbAXyBYIR4oOM0knu1BDuzvQcvM911GBFQCUq+Cn5zeCFBKWqj3YWrEZgBPAb8H15mneM8IjtQCUp+88oSBLNJLySYbdrXbSBphyrgb8CfgD/jZdY4ziPSIpWgFA6vrJRgRZpy4ERgN7eBJMv7BKX3J+CfeJltjvOItItKUAqTV2bARIIyPBH4NNDPaaZ48YGZbC8+LzPXcR6RTlEJSjR4ZcXAFD4pxSPRPg9zrZpgl0V/AtJ4mdWO84jsMpWgRFMw6/RYPinFg9DPLzrCBxYDs3b48zI1TlOJ5JhKUOLBKxsEHA8cBUwI//Z0mim/LGPHwpuNl9nsNpJI11MJSnx5ZWXAeD4pxe1/Uf9JxnJ2LrwNbiOJuKESFGnKKxtAUIZNC3IPl7E6aA1Q2eRvGTAHL7PWVSiRfKMSFGkvr2wgQREODv+GZB1u7rRB5H7nwpuA1S38rSIY5VXiZbbm+HZFIkklKNJVgp9xlPFJMQ4kWOGkvslfXTtP24aXqe3eOyESbSpBERGJLa0yLiIisaUSFBGR2FIJiohIbKkERUQktlSCIiISWypBERGJLZWgiIjElkpQRERiSyUoIiKxpRIUEZHYUgmKxIiZVbnO0Bwz88zsW65zSPyoBEVkl5hZsesMIp2lEhSJKDN70sxmm9lCM7sy6/Q7wtP+bmZDw9NeMLNbzWyGmS02s2PD03ub2TQzm29mc8zs+PD0i83saTP7B/D38PiTZvacmVWa2TVm9o3wOq+a2aDweleY2Uwzm2tmj5tZqYOHRuRjKkGR6LrU9/3JwKHAtWY2GOgDzPJ9fwIwHfhh1uWLfd+fAlyXdfrVgO/7/gHA54H/M7Pe4XmHAOf4vv/p8PhE4HPAYcCPgWrf9w8G/g1cFF7mCd/3D/N9fxKwCLgs5/dapANUgiLRda2ZzQVeBUYC+wGNwCPh+Q8Ax2Rd/onw/2wgGR4+Jrwcvu+/RbDT3jHhec/5vv9R1vX/6fv+Zt/31wIZ4E/h6fOzpjfRzF40s/nA+cCEXbyPIrtE8/JFIsjMjgOmAkf6vl9tZi8AvZu5aPYORbeF/xto32fDlibHt2Udbsw63pg1vfuBz/q+P9fMLgaOa8ftiHQZjQRFoqkM2BAW4P7AEeHpCeCc8PAXgJfamM6LBCM2zGwMsBfw9i7k6gesMrMe26cr4pJKUCSangWKzWwRUEEwSxSC0dsUM1sAnADc2MZ0fgkkwtmXjwAX+76/rY3rtOb7wGvAy8BbuzAdkZww3/fbvpSIiEgEaSQoIiKxpRIUEZHYUgmKiEhsqQRFRCS2VIIiIhJbKkEREYktlaCIiMSWSlBERGJLJSgiIrGlEhQRkdhSCYqISGypBEVEJLZUgiIiElsqQRERiS2VoIiIxJZKUEREYkslKCIisaUSFBGR2FIJiohIbKkERUQktlSCIiISWypBERGJLZWgiIjElkpQRERiSyUoIiKxpRIUEZHYUgmKiEhsqQRFRCS2VIIiIhJbKkEREYmt/x9mtGagZziImQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a dataframe with only numeric attributes of binary class dataset and encoded label attribute \n",
        "numeric_bin = bin_data[numeric_col]\n",
        "numeric_bin['intrusion'] = bin_data['intrusion']\n",
        "\n",
        "# finding the attributes which have more than 0.5 correlation with encoded attack label attribute \n",
        "corr= numeric_bin.corr()\n",
        "corr_y = abs(corr['intrusion'])\n",
        "highest_corr = corr_y[corr_y >0.5]\n",
        "highest_corr.sort_values(ascending=True)\n",
        "\n",
        "# selecting attributes found by using pearson correlation coefficient\n",
        "numeric_bin = bin_data[['count','srv_serror_rate','serror_rate','dst_host_serror_rate','dst_host_srv_serror_rate',\n",
        "                         'logged_in','dst_host_same_srv_rate','dst_host_srv_count','same_srv_rate']]\n",
        "\n",
        "# joining the selected attribute with the one-hot-encoded categorical dataframe\n",
        "numeric_bin = numeric_bin.join(categorical)\n",
        "# then joining encoded, one-hot-encoded, and original attack label attribute\n",
        "bin_data = numeric_bin.join(bin_data[['intrusion','abnormal','normal','label']])\n",
        "\n",
        "bin_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "rHNJcUkzkG-l",
        "outputId": "63b142ab-2840-4b56-b4ad-7c0285c1dd91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           count  srv_serror_rate  serror_rate  dst_host_serror_rate  \\\n",
              "0      -0.717045        -0.631929    -0.637209             -0.639532   \n",
              "1      -0.620982        -0.631929    -0.637209             -0.639532   \n",
              "2       0.339648         1.605104     1.602664              1.608759   \n",
              "3      -0.690846        -0.184522    -0.189235             -0.572083   \n",
              "4      -0.472521        -0.631929    -0.637209             -0.639532   \n",
              "...          ...              ...          ...                   ...   \n",
              "125968  0.872361         1.605104     1.602664              1.608759   \n",
              "125969 -0.717045        -0.631929    -0.637209             -0.639532   \n",
              "125970 -0.725778        -0.631929    -0.637209              0.979238   \n",
              "125971  0.523041         1.605104     1.602664              1.608759   \n",
              "125972 -0.725778        -0.631929    -0.637209             -0.639532   \n",
              "\n",
              "        dst_host_srv_serror_rate  logged_in  dst_host_same_srv_rate  \\\n",
              "0                      -0.624871  -0.809262               -0.782367   \n",
              "1                      -0.624871  -0.809262               -1.161030   \n",
              "2                       1.618955  -0.809262               -0.938287   \n",
              "3                      -0.602433   1.235694                1.066401   \n",
              "4                      -0.624871   1.235694                1.066401   \n",
              "...                          ...        ...                     ...   \n",
              "125968                  1.618955  -0.809262               -0.938287   \n",
              "125969                 -0.624871  -0.809262                0.977304   \n",
              "125970                 -0.624871   1.235694               -0.893738   \n",
              "125971                  1.618955  -0.809262               -1.094207   \n",
              "125972                 -0.624871   1.235694               -0.492801   \n",
              "\n",
              "        dst_host_srv_count  same_srv_rate  protocol_type_icmp  ...  flag_S0  \\\n",
              "0                -0.818890       0.771283                   0  ...        0   \n",
              "1                -1.035688      -1.321428                   0  ...        0   \n",
              "2                -0.809857      -1.389669                   0  ...        1   \n",
              "3                 1.258754       0.771283                   0  ...        0   \n",
              "4                 1.258754       0.771283                   0  ...        0   \n",
              "...                    ...            ...                 ...  ...      ...   \n",
              "125968           -0.818890      -1.184947                   0  ...        1   \n",
              "125969            1.159389       0.771283                   0  ...        0   \n",
              "125970           -0.773724       0.771283                   0  ...        0   \n",
              "125971           -0.972455      -1.366922                   0  ...        1   \n",
              "125972           -0.349162       0.771283                   0  ...        0   \n",
              "\n",
              "        flag_S1  flag_S2  flag_S3  flag_SF  flag_SH  intrusion  abnormal  \\\n",
              "0             0        0        0        1        0          1         0   \n",
              "1             0        0        0        1        0          1         0   \n",
              "2             0        0        0        0        0          0         1   \n",
              "3             0        0        0        1        0          1         0   \n",
              "4             0        0        0        1        0          1         0   \n",
              "...         ...      ...      ...      ...      ...        ...       ...   \n",
              "125968        0        0        0        0        0          0         1   \n",
              "125969        0        0        0        1        0          1         0   \n",
              "125970        0        0        0        1        0          1         0   \n",
              "125971        0        0        0        0        0          0         1   \n",
              "125972        0        0        0        1        0          1         0   \n",
              "\n",
              "        normal     label  \n",
              "0            1    normal  \n",
              "1            1    normal  \n",
              "2            0  abnormal  \n",
              "3            1    normal  \n",
              "4            1    normal  \n",
              "...        ...       ...  \n",
              "125968       0  abnormal  \n",
              "125969       1    normal  \n",
              "125970       1    normal  \n",
              "125971       0  abnormal  \n",
              "125972       1    normal  \n",
              "\n",
              "[125973 rows x 97 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dcfe4949-4937-4901-8a95-3be5649b690a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>srv_serror_rate</th>\n",
              "      <th>serror_rate</th>\n",
              "      <th>dst_host_serror_rate</th>\n",
              "      <th>dst_host_srv_serror_rate</th>\n",
              "      <th>logged_in</th>\n",
              "      <th>dst_host_same_srv_rate</th>\n",
              "      <th>dst_host_srv_count</th>\n",
              "      <th>same_srv_rate</th>\n",
              "      <th>protocol_type_icmp</th>\n",
              "      <th>...</th>\n",
              "      <th>flag_S0</th>\n",
              "      <th>flag_S1</th>\n",
              "      <th>flag_S2</th>\n",
              "      <th>flag_S3</th>\n",
              "      <th>flag_SF</th>\n",
              "      <th>flag_SH</th>\n",
              "      <th>intrusion</th>\n",
              "      <th>abnormal</th>\n",
              "      <th>normal</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.717045</td>\n",
              "      <td>-0.631929</td>\n",
              "      <td>-0.637209</td>\n",
              "      <td>-0.639532</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>-0.809262</td>\n",
              "      <td>-0.782367</td>\n",
              "      <td>-0.818890</td>\n",
              "      <td>0.771283</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.620982</td>\n",
              "      <td>-0.631929</td>\n",
              "      <td>-0.637209</td>\n",
              "      <td>-0.639532</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>-0.809262</td>\n",
              "      <td>-1.161030</td>\n",
              "      <td>-1.035688</td>\n",
              "      <td>-1.321428</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.339648</td>\n",
              "      <td>1.605104</td>\n",
              "      <td>1.602664</td>\n",
              "      <td>1.608759</td>\n",
              "      <td>1.618955</td>\n",
              "      <td>-0.809262</td>\n",
              "      <td>-0.938287</td>\n",
              "      <td>-0.809857</td>\n",
              "      <td>-1.389669</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>abnormal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.690846</td>\n",
              "      <td>-0.184522</td>\n",
              "      <td>-0.189235</td>\n",
              "      <td>-0.572083</td>\n",
              "      <td>-0.602433</td>\n",
              "      <td>1.235694</td>\n",
              "      <td>1.066401</td>\n",
              "      <td>1.258754</td>\n",
              "      <td>0.771283</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.472521</td>\n",
              "      <td>-0.631929</td>\n",
              "      <td>-0.637209</td>\n",
              "      <td>-0.639532</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>1.235694</td>\n",
              "      <td>1.066401</td>\n",
              "      <td>1.258754</td>\n",
              "      <td>0.771283</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125968</th>\n",
              "      <td>0.872361</td>\n",
              "      <td>1.605104</td>\n",
              "      <td>1.602664</td>\n",
              "      <td>1.608759</td>\n",
              "      <td>1.618955</td>\n",
              "      <td>-0.809262</td>\n",
              "      <td>-0.938287</td>\n",
              "      <td>-0.818890</td>\n",
              "      <td>-1.184947</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>abnormal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125969</th>\n",
              "      <td>-0.717045</td>\n",
              "      <td>-0.631929</td>\n",
              "      <td>-0.637209</td>\n",
              "      <td>-0.639532</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>-0.809262</td>\n",
              "      <td>0.977304</td>\n",
              "      <td>1.159389</td>\n",
              "      <td>0.771283</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125970</th>\n",
              "      <td>-0.725778</td>\n",
              "      <td>-0.631929</td>\n",
              "      <td>-0.637209</td>\n",
              "      <td>0.979238</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>1.235694</td>\n",
              "      <td>-0.893738</td>\n",
              "      <td>-0.773724</td>\n",
              "      <td>0.771283</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125971</th>\n",
              "      <td>0.523041</td>\n",
              "      <td>1.605104</td>\n",
              "      <td>1.602664</td>\n",
              "      <td>1.608759</td>\n",
              "      <td>1.618955</td>\n",
              "      <td>-0.809262</td>\n",
              "      <td>-1.094207</td>\n",
              "      <td>-0.972455</td>\n",
              "      <td>-1.366922</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>abnormal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125972</th>\n",
              "      <td>-0.725778</td>\n",
              "      <td>-0.631929</td>\n",
              "      <td>-0.637209</td>\n",
              "      <td>-0.639532</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>1.235694</td>\n",
              "      <td>-0.492801</td>\n",
              "      <td>-0.349162</td>\n",
              "      <td>0.771283</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125973 rows Ã— 97 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dcfe4949-4937-4901-8a95-3be5649b690a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dcfe4949-4937-4901-8a95-3be5649b690a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dcfe4949-4937-4901-8a95-3be5649b690a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = bin_data.iloc[:,0:93].to_numpy() # dataset excluding target attribute (encoded, one-hot-encoded,original)\n",
        "Y = bin_data['intrusion'] # target attribute\n",
        "\n",
        "# splitting the dataset 75% for training and 25% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "wcAekdwckG8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "bwd-qJt3h4te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using kernel as linear \n",
        "lsvm = SVC(kernel='linear',gamma='auto') \n",
        "lsvm.fit(X_train,y_train) # training model on training dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCOuXaz1hpna",
        "outputId": "ab78c096-340d-4b7b-9511-424626da4b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(gamma='auto', kernel='linear')"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = lsvm.predict(X_test) # predicting target attribute on testing dataset\n",
        "ac = accuracy_score(y_test, y_pred)*100 # calculating accuracy of predicted data\n",
        "print(\"LSVM-Classifier Binary Set-Accuracy is \", ac)\n",
        "\n",
        "# classification report\n",
        "print(classification_report(y_test, y_pred,target_names=le1.classes_))\n",
        "\n",
        "print(\"Mean Absolute Error - \" , metrics.mean_absolute_error(y_test, y_pred))\n",
        "print(\"Mean Squared Error - \" , metrics.mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error - \" , np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "print(\"R2 Score - \" , metrics.explained_variance_score(y_test, y_pred)*100)\n",
        "print(\"Accuracy - \",accuracy_score(y_test,y_pred)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwlDr57Rhpks",
        "outputId": "e673760e-b3b6-4f16-eddb-67a86be32414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSVM-Classifier Binary Set-Accuracy is  96.69778370483266\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    abnormal       0.97      0.96      0.96     14720\n",
            "      normal       0.96      0.97      0.97     16774\n",
            "\n",
            "    accuracy                           0.97     31494\n",
            "   macro avg       0.97      0.97      0.97     31494\n",
            "weighted avg       0.97      0.97      0.97     31494\n",
            "\n",
            "Mean Absolute Error -  0.03302216295167333\n",
            "Mean Squared Error -  0.03302216295167333\n",
            "Root Mean Squared Error -  0.1817200125238641\n",
            "R2 Score -  86.74560396265441\n",
            "Accuracy -  96.69778370483266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSVM"
      ],
      "metadata": {
        "id": "cnxJpyofh6pH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qsvm=SVC(kernel='poly',gamma='auto') # using kernal as polynomial for quadratic svm\n",
        "qsvm.fit(X_train,y_train) # training model on training dataset\n",
        "y_pred=qsvm.predict(X_test) # predicting target attribute on testing dataset\n",
        "ac=accuracy_score(y_test, y_pred)*100 # calculating accuracy of predicted data\n",
        "print(\"QSVM-Classifier Binary Set-Accuracy is \", ac)\n",
        "\n",
        "# classification report\n",
        "print(classification_report(y_test, y_pred,target_names=le1.classes_))\n",
        "\n",
        "print(\"Mean Absolute Error - \" , metrics.mean_absolute_error(y_test, y_pred))\n",
        "print(\"Mean Squared Error - \" , metrics.mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error - \" , np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "print(\"R2 Score - \" , metrics.explained_variance_score(y_test, y_pred)*100)\n",
        "print(\"Accuracy - \",accuracy_score(y_test,y_pred)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVB9PI07hphv",
        "outputId": "7016fe2b-30ef-43f4-9a91-7ce21101c678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QSVM-Classifier Binary Set-Accuracy is  95.71029402425859\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    abnormal       0.99      0.92      0.95     14720\n",
            "      normal       0.93      0.99      0.96     16774\n",
            "\n",
            "    accuracy                           0.96     31494\n",
            "   macro avg       0.96      0.95      0.96     31494\n",
            "weighted avg       0.96      0.96      0.96     31494\n",
            "\n",
            "Mean Absolute Error -  0.04289705975741411\n",
            "Mean Squared Error -  0.04289705975741411\n",
            "Root Mean Squared Error -  0.20711605383797296\n",
            "R2 Score -  83.24290009836652\n",
            "Accuracy -  95.71029402425859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA"
      ],
      "metadata": {
        "id": "q1e-hzR_iLB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LinearDiscriminantAnalysis() \n",
        "lda.fit(X_train, y_train)  # training model on training dataset\n",
        "\n",
        "y_pred = lda.predict(X_test) # predicting target attribute on testing dataset\n",
        "ac=accuracy_score(y_test, y_pred)*100 # calculating accuracy of predicted data\n",
        "print(\"LDA-Classifier Set-Accuracy is \", ac)\n",
        "\n",
        "# classification report\n",
        "print(classification_report(y_test, y_pred,target_names=le1.classes_))\n",
        "\n",
        "print(\"Mean Absolute Error - \" , metrics.mean_absolute_error(y_test, y_pred))\n",
        "print(\"Mean Squared Error - \" , metrics.mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error - \" , np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "print(\"R2 Score - \" , metrics.explained_variance_score(y_test, y_pred)*100)\n",
        "print(\"Accuracy - \",accuracy_score(y_test,y_pred)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlHXgaOChpfR",
        "outputId": "1af449a1-65a1-417a-bbc8-d5f50f8fde60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA-Classifier Set-Accuracy is  96.70730932876104\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    abnormal       0.97      0.96      0.96     14720\n",
            "      normal       0.96      0.98      0.97     16774\n",
            "\n",
            "    accuracy                           0.97     31494\n",
            "   macro avg       0.97      0.97      0.97     31494\n",
            "weighted avg       0.97      0.97      0.97     31494\n",
            "\n",
            "Mean Absolute Error -  0.03292690671238966\n",
            "Mean Squared Error -  0.03292690671238966\n",
            "Root Mean Squared Error -  0.1814577270671868\n",
            "R2 Score -  86.8001441639753\n",
            "Accuracy -  96.70730932876104\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QDA"
      ],
      "metadata": {
        "id": "IrgIIpxviNxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qda = QuadraticDiscriminantAnalysis()\n",
        "qda.fit(X_train, y_train) # training model on training dataset\n",
        "\n",
        "y_pred = qda.predict(X_test) # predicting target attribute on testing dataset\n",
        "ac=accuracy_score(y_test, y_pred)*100  # calculating accuracy of predicted data\n",
        "print(\"QDA-Classifier Set-Accuracy is \", ac)\n",
        "\n",
        "# classification report\n",
        "print(classification_report(y_test, y_pred,target_names=le1.classes_))\n",
        "\n",
        "print(\"Mean Absolute Error - \" , metrics.mean_absolute_error(y_test, y_pred))\n",
        "print(\"Mean Squared Error - \" , metrics.mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error - \" , np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "print(\"R2 Score - \" , metrics.explained_variance_score(y_test, y_pred)*100)\n",
        "print(\"Accuracy - \",accuracy_score(y_test,y_pred)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAOGBYufhpcg",
        "outputId": "f00e1b27-abbb-4e7e-c6dd-c372e56f2f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QDA-Classifier Set-Accuracy is  68.84168413031053\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    abnormal       1.00      0.33      0.50     14720\n",
            "      normal       0.63      1.00      0.77     16774\n",
            "\n",
            "    accuracy                           0.69     31494\n",
            "   macro avg       0.81      0.67      0.64     31494\n",
            "weighted avg       0.80      0.69      0.65     31494\n",
            "\n",
            "Mean Absolute Error -  0.3115831586968946\n",
            "Mean Squared Error -  0.3115831586968946\n",
            "Root Mean Squared Error -  0.5581963442167054\n",
            "R2 Score -  13.69091326315054\n",
            "Accuracy -  68.84168413031053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceptron"
      ],
      "metadata": {
        "id": "OjD7XQ-LiWIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# X = bin_data.iloc[:,0:93].values # dataset excluding target attribute (encoded, one-hot-encoded,original)\n",
        "# Y = bin_data[['intrusion']].values # target attribute\n",
        "\n",
        "# splitting the dataset 75% for training and 25% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.25, random_state=42)\n",
        "\n",
        "mlp = Sequential() # creating model\n",
        "\n",
        "# adding input layer and first layer with 50 neurons\n",
        "mlp.add(Dense(units=50, input_dim=X_train.shape[1], activation='relu'))\n",
        "# output layer with sigmoid activation\n",
        "mlp.add(Dense(units=1,activation='sigmoid'))\n",
        "\n",
        "# defining loss function, optimizer, metrics and then compiling model\n",
        "mlp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# summary of model layers\n",
        "mlp.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp3dtIh5jI_e",
        "outputId": "40dd201c-561a-4835-9d59-7954710952e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_8 (Dense)             (None, 50)                4700      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,751\n",
            "Trainable params: 4,751\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model on training dataset\n",
        "history = mlp.fit(X_train, y_train, epochs=100, batch_size=5000,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6eNnap8jLTG",
        "outputId": "35e80178-e993-420a-cec3-81616974b84f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 0.5767 - accuracy: 0.7382 - val_loss: 0.4353 - val_accuracy: 0.8851\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3638 - accuracy: 0.9057 - val_loss: 0.2957 - val_accuracy: 0.9204\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2634 - accuracy: 0.9190 - val_loss: 0.2324 - val_accuracy: 0.9233\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.2158 - accuracy: 0.9220 - val_loss: 0.1994 - val_accuracy: 0.9346\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1885 - accuracy: 0.9432 - val_loss: 0.1783 - val_accuracy: 0.9544\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1698 - accuracy: 0.9580 - val_loss: 0.1630 - val_accuracy: 0.9636\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1558 - accuracy: 0.9643 - val_loss: 0.1512 - val_accuracy: 0.9664\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1447 - accuracy: 0.9670 - val_loss: 0.1417 - val_accuracy: 0.9684\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1360 - accuracy: 0.9692 - val_loss: 0.1342 - val_accuracy: 0.9690\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1290 - accuracy: 0.9696 - val_loss: 0.1281 - val_accuracy: 0.9690\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1234 - accuracy: 0.9697 - val_loss: 0.1231 - val_accuracy: 0.9689\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1187 - accuracy: 0.9697 - val_loss: 0.1188 - val_accuracy: 0.9691\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1146 - accuracy: 0.9697 - val_loss: 0.1150 - val_accuracy: 0.9694\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1111 - accuracy: 0.9699 - val_loss: 0.1117 - val_accuracy: 0.9697\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1079 - accuracy: 0.9702 - val_loss: 0.1087 - val_accuracy: 0.9700\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1050 - accuracy: 0.9703 - val_loss: 0.1060 - val_accuracy: 0.9702\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1025 - accuracy: 0.9704 - val_loss: 0.1036 - val_accuracy: 0.9702\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1003 - accuracy: 0.9707 - val_loss: 0.1014 - val_accuracy: 0.9702\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0982 - accuracy: 0.9710 - val_loss: 0.0995 - val_accuracy: 0.9705\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0964 - accuracy: 0.9711 - val_loss: 0.0977 - val_accuracy: 0.9704\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0948 - accuracy: 0.9712 - val_loss: 0.0961 - val_accuracy: 0.9704\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0932 - accuracy: 0.9713 - val_loss: 0.0947 - val_accuracy: 0.9705\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0919 - accuracy: 0.9715 - val_loss: 0.0934 - val_accuracy: 0.9706\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0906 - accuracy: 0.9715 - val_loss: 0.0921 - val_accuracy: 0.9706\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0894 - accuracy: 0.9716 - val_loss: 0.0910 - val_accuracy: 0.9706\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0883 - accuracy: 0.9717 - val_loss: 0.0899 - val_accuracy: 0.9706\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0873 - accuracy: 0.9717 - val_loss: 0.0889 - val_accuracy: 0.9707\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0864 - accuracy: 0.9718 - val_loss: 0.0879 - val_accuracy: 0.9708\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0855 - accuracy: 0.9716 - val_loss: 0.0870 - val_accuracy: 0.9708\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0846 - accuracy: 0.9717 - val_loss: 0.0862 - val_accuracy: 0.9708\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0838 - accuracy: 0.9718 - val_loss: 0.0854 - val_accuracy: 0.9708\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0831 - accuracy: 0.9718 - val_loss: 0.0847 - val_accuracy: 0.9708\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0824 - accuracy: 0.9719 - val_loss: 0.0840 - val_accuracy: 0.9708\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0817 - accuracy: 0.9719 - val_loss: 0.0833 - val_accuracy: 0.9709\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0810 - accuracy: 0.9719 - val_loss: 0.0826 - val_accuracy: 0.9709\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0804 - accuracy: 0.9720 - val_loss: 0.0820 - val_accuracy: 0.9710\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0798 - accuracy: 0.9720 - val_loss: 0.0814 - val_accuracy: 0.9711\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0792 - accuracy: 0.9719 - val_loss: 0.0808 - val_accuracy: 0.9710\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0787 - accuracy: 0.9720 - val_loss: 0.0802 - val_accuracy: 0.9710\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0781 - accuracy: 0.9720 - val_loss: 0.0797 - val_accuracy: 0.9705\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0776 - accuracy: 0.9720 - val_loss: 0.0791 - val_accuracy: 0.9706\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0771 - accuracy: 0.9720 - val_loss: 0.0786 - val_accuracy: 0.9705\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0765 - accuracy: 0.9720 - val_loss: 0.0780 - val_accuracy: 0.9705\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0760 - accuracy: 0.9721 - val_loss: 0.0776 - val_accuracy: 0.9707\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0755 - accuracy: 0.9725 - val_loss: 0.0771 - val_accuracy: 0.9708\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0751 - accuracy: 0.9725 - val_loss: 0.0767 - val_accuracy: 0.9708\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0746 - accuracy: 0.9726 - val_loss: 0.0762 - val_accuracy: 0.9709\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0742 - accuracy: 0.9726 - val_loss: 0.0758 - val_accuracy: 0.9711\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0738 - accuracy: 0.9732 - val_loss: 0.0754 - val_accuracy: 0.9717\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0734 - accuracy: 0.9736 - val_loss: 0.0750 - val_accuracy: 0.9717\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0730 - accuracy: 0.9737 - val_loss: 0.0746 - val_accuracy: 0.9723\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0727 - accuracy: 0.9740 - val_loss: 0.0742 - val_accuracy: 0.9723\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0723 - accuracy: 0.9743 - val_loss: 0.0739 - val_accuracy: 0.9727\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0720 - accuracy: 0.9745 - val_loss: 0.0736 - val_accuracy: 0.9732\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0717 - accuracy: 0.9754 - val_loss: 0.0732 - val_accuracy: 0.9736\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0713 - accuracy: 0.9753 - val_loss: 0.0730 - val_accuracy: 0.9735\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0710 - accuracy: 0.9755 - val_loss: 0.0727 - val_accuracy: 0.9741\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0707 - accuracy: 0.9757 - val_loss: 0.0724 - val_accuracy: 0.9742\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0705 - accuracy: 0.9757 - val_loss: 0.0721 - val_accuracy: 0.9743\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0702 - accuracy: 0.9757 - val_loss: 0.0719 - val_accuracy: 0.9745\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0699 - accuracy: 0.9760 - val_loss: 0.0717 - val_accuracy: 0.9747\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0697 - accuracy: 0.9760 - val_loss: 0.0715 - val_accuracy: 0.9747\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0695 - accuracy: 0.9760 - val_loss: 0.0712 - val_accuracy: 0.9748\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0692 - accuracy: 0.9766 - val_loss: 0.0709 - val_accuracy: 0.9749\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0690 - accuracy: 0.9764 - val_loss: 0.0708 - val_accuracy: 0.9748\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0688 - accuracy: 0.9763 - val_loss: 0.0705 - val_accuracy: 0.9751\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0685 - accuracy: 0.9769 - val_loss: 0.0703 - val_accuracy: 0.9753\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0684 - accuracy: 0.9772 - val_loss: 0.0702 - val_accuracy: 0.9752\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0681 - accuracy: 0.9766 - val_loss: 0.0700 - val_accuracy: 0.9754\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0679 - accuracy: 0.9772 - val_loss: 0.0698 - val_accuracy: 0.9754\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0677 - accuracy: 0.9774 - val_loss: 0.0695 - val_accuracy: 0.9756\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0675 - accuracy: 0.9771 - val_loss: 0.0694 - val_accuracy: 0.9753\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0673 - accuracy: 0.9773 - val_loss: 0.0693 - val_accuracy: 0.9756\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0672 - accuracy: 0.9775 - val_loss: 0.0691 - val_accuracy: 0.9756\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0670 - accuracy: 0.9773 - val_loss: 0.0690 - val_accuracy: 0.9754\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0668 - accuracy: 0.9775 - val_loss: 0.0688 - val_accuracy: 0.9757\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0666 - accuracy: 0.9775 - val_loss: 0.0686 - val_accuracy: 0.9757\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0665 - accuracy: 0.9777 - val_loss: 0.0685 - val_accuracy: 0.9758\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0663 - accuracy: 0.9777 - val_loss: 0.0684 - val_accuracy: 0.9758\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0662 - accuracy: 0.9779 - val_loss: 0.0682 - val_accuracy: 0.9760\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0660 - accuracy: 0.9780 - val_loss: 0.0681 - val_accuracy: 0.9759\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0659 - accuracy: 0.9780 - val_loss: 0.0680 - val_accuracy: 0.9760\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0657 - accuracy: 0.9780 - val_loss: 0.0678 - val_accuracy: 0.9759\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0656 - accuracy: 0.9780 - val_loss: 0.0677 - val_accuracy: 0.9759\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0655 - accuracy: 0.9780 - val_loss: 0.0676 - val_accuracy: 0.9759\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0654 - accuracy: 0.9780 - val_loss: 0.0675 - val_accuracy: 0.9759\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0652 - accuracy: 0.9781 - val_loss: 0.0674 - val_accuracy: 0.9759\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0651 - accuracy: 0.9782 - val_loss: 0.0672 - val_accuracy: 0.9758\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0650 - accuracy: 0.9782 - val_loss: 0.0672 - val_accuracy: 0.9758\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0649 - accuracy: 0.9781 - val_loss: 0.0671 - val_accuracy: 0.9760\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0648 - accuracy: 0.9782 - val_loss: 0.0670 - val_accuracy: 0.9761\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0647 - accuracy: 0.9782 - val_loss: 0.0669 - val_accuracy: 0.9760\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0645 - accuracy: 0.9782 - val_loss: 0.0669 - val_accuracy: 0.9761\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0645 - accuracy: 0.9782 - val_loss: 0.0668 - val_accuracy: 0.9760\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0644 - accuracy: 0.9783 - val_loss: 0.0667 - val_accuracy: 0.9761\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0642 - accuracy: 0.9784 - val_loss: 0.0665 - val_accuracy: 0.9762\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0641 - accuracy: 0.9783 - val_loss: 0.0665 - val_accuracy: 0.9762\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0641 - accuracy: 0.9784 - val_loss: 0.0663 - val_accuracy: 0.9762\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0640 - accuracy: 0.9785 - val_loss: 0.0662 - val_accuracy: 0.9762\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0639 - accuracy: 0.9785 - val_loss: 0.0662 - val_accuracy: 0.9763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining loss function, optimizer, metrics and then compiling model\n",
        "mlp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# predicting target attribute on testing dataset\n",
        "test_results = mlp.evaluate(X_test, y_test, verbose=1)\n",
        "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}')\n",
        "\n",
        "y_pred = mlp.predict(X_test).ravel()\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "auc = auc(fpr, tpr)\n",
        "\n",
        "pred = mlp.predict(X_test)\n",
        "y_classes = (mlp.predict(X_test)>0.5).astype('int32')\n",
        "\n",
        "print(\"Recall Score - \",recall_score(y_test,y_classes))\n",
        "print(\"F1 Score - \",f1_score(y_test,y_classes))\n",
        "print(\"Precision Score - \",precision_score(y_test,y_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-LbFohmhpZ5",
        "outputId": "afdb5e5f-1cc6-47c2-e4e4-f3b4ad3f3093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "985/985 [==============================] - 1s 931us/step - loss: 0.0647 - accuracy: 0.9780\n",
            "Test results - Loss: 0.06468357145786285 - Accuracy: 97.79958128929138\n",
            "Recall Score -  0.9867056158340288\n",
            "F1 Score -  0.9794940079893475\n",
            "Precision Score -  0.9723870512895835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AdhEQyHOhpXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3UFq39o9hpUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ada-grad"
      ],
      "metadata": {
        "id": "BzCTmvEYrNe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = bin_data.iloc[:,0:93].values # dataset excluding target attribute (encoded, one-hot-encoded,original)\n",
        "Y1 = bin_data[['intrusion']].values # target attribute\n",
        "\n",
        "# X = bin_data.iloc[:,0:93].to_numpy() # dataset excluding target attribute (encoded, one-hot-encoded,original)\n",
        "# Y = bin_data['intrusion'] # target attribute\n",
        "\n",
        "# splitting the dataset 75% for training and 25% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X1,Y1, test_size=0.25, random_state=42)\n",
        "\n",
        "mlp1 = Sequential() # creating model\n",
        "\n",
        "# adding input layer and first layer with 50 neurons\n",
        "mlp1.add(Dense(units=50, input_dim=X_train.shape[1], activation='relu'))\n",
        "# output layer with sigmoid activation\n",
        "mlp1.add(Dense(units=1,activation='sigmoid'))\n",
        "\n",
        "# defining loss function, optimizer, metrics and then compiling model\n",
        "mlp1.compile(loss='binary_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
        "\n",
        "# summary of model layers\n",
        "mlp1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IncXN6xXhpR8",
        "outputId": "4331836a-ab42-4abf-f27b-79451dfe1a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_26 (Dense)            (None, 50)                4700      \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,751\n",
            "Trainable params: 4,751\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model on training dataset\n",
        "history = mlp1.fit(X_train, y_train, epochs=100, batch_size=5000,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2noeMxBnEH9",
        "outputId": "27e113d4-5249-4bbe-8e99-8db0b616bad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 0.7423 - accuracy: 0.5336 - val_loss: 0.7125 - val_accuracy: 0.5362\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.6955 - accuracy: 0.5382 - val_loss: 0.6735 - val_accuracy: 0.5533\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6605 - accuracy: 0.5863 - val_loss: 0.6427 - val_accuracy: 0.6347\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6324 - accuracy: 0.6573 - val_loss: 0.6173 - val_accuracy: 0.6866\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.6087 - accuracy: 0.7061 - val_loss: 0.5954 - val_accuracy: 0.7358\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.5881 - accuracy: 0.7500 - val_loss: 0.5762 - val_accuracy: 0.7701\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.5700 - accuracy: 0.7770 - val_loss: 0.5593 - val_accuracy: 0.7947\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.5539 - accuracy: 0.7981 - val_loss: 0.5441 - val_accuracy: 0.8098\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.5394 - accuracy: 0.8078 - val_loss: 0.5305 - val_accuracy: 0.8124\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.5264 - accuracy: 0.8090 - val_loss: 0.5181 - val_accuracy: 0.8134\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.5145 - accuracy: 0.8102 - val_loss: 0.5068 - val_accuracy: 0.8146\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.5036 - accuracy: 0.8117 - val_loss: 0.4965 - val_accuracy: 0.8160\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.4937 - accuracy: 0.8128 - val_loss: 0.4869 - val_accuracy: 0.8166\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4844 - accuracy: 0.8132 - val_loss: 0.4780 - val_accuracy: 0.8170\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.4758 - accuracy: 0.8136 - val_loss: 0.4697 - val_accuracy: 0.8174\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4677 - accuracy: 0.8141 - val_loss: 0.4620 - val_accuracy: 0.8181\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.4602 - accuracy: 0.8154 - val_loss: 0.4547 - val_accuracy: 0.8236\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.4531 - accuracy: 0.8244 - val_loss: 0.4478 - val_accuracy: 0.8347\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.4464 - accuracy: 0.8350 - val_loss: 0.4414 - val_accuracy: 0.8432\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.4401 - accuracy: 0.8413 - val_loss: 0.4353 - val_accuracy: 0.8467\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.4341 - accuracy: 0.8441 - val_loss: 0.4295 - val_accuracy: 0.8485\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4285 - accuracy: 0.8460 - val_loss: 0.4240 - val_accuracy: 0.8505\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4231 - accuracy: 0.8476 - val_loss: 0.4188 - val_accuracy: 0.8526\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4180 - accuracy: 0.8502 - val_loss: 0.4138 - val_accuracy: 0.8561\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.4130 - accuracy: 0.8528 - val_loss: 0.4089 - val_accuracy: 0.8585\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.4083 - accuracy: 0.8556 - val_loss: 0.4043 - val_accuracy: 0.8625\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.4037 - accuracy: 0.8586 - val_loss: 0.3999 - val_accuracy: 0.8642\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3994 - accuracy: 0.8602 - val_loss: 0.3957 - val_accuracy: 0.8651\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3952 - accuracy: 0.8614 - val_loss: 0.3916 - val_accuracy: 0.8664\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3912 - accuracy: 0.8629 - val_loss: 0.3877 - val_accuracy: 0.8687\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3874 - accuracy: 0.8649 - val_loss: 0.3840 - val_accuracy: 0.8700\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3837 - accuracy: 0.8665 - val_loss: 0.3804 - val_accuracy: 0.8715\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3801 - accuracy: 0.8677 - val_loss: 0.3769 - val_accuracy: 0.8727\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.3767 - accuracy: 0.8694 - val_loss: 0.3735 - val_accuracy: 0.8742\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.3734 - accuracy: 0.8704 - val_loss: 0.3703 - val_accuracy: 0.8751\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.3702 - accuracy: 0.8710 - val_loss: 0.3672 - val_accuracy: 0.8757\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 11ms/step - loss: 0.3671 - accuracy: 0.8719 - val_loss: 0.3642 - val_accuracy: 0.8764\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3642 - accuracy: 0.8727 - val_loss: 0.3613 - val_accuracy: 0.8769\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3613 - accuracy: 0.8733 - val_loss: 0.3585 - val_accuracy: 0.8776\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3585 - accuracy: 0.8742 - val_loss: 0.3558 - val_accuracy: 0.8781\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.3559 - accuracy: 0.8750 - val_loss: 0.3532 - val_accuracy: 0.8787\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.3533 - accuracy: 0.8757 - val_loss: 0.3507 - val_accuracy: 0.8790\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 12ms/step - loss: 0.3507 - accuracy: 0.8765 - val_loss: 0.3482 - val_accuracy: 0.8803\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3483 - accuracy: 0.8787 - val_loss: 0.3458 - val_accuracy: 0.8828\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3459 - accuracy: 0.8813 - val_loss: 0.3435 - val_accuracy: 0.8852\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3436 - accuracy: 0.8826 - val_loss: 0.3412 - val_accuracy: 0.8854\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3414 - accuracy: 0.8827 - val_loss: 0.3390 - val_accuracy: 0.8856\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3392 - accuracy: 0.8827 - val_loss: 0.3369 - val_accuracy: 0.8857\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3371 - accuracy: 0.8826 - val_loss: 0.3348 - val_accuracy: 0.8855\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.3350 - accuracy: 0.8825 - val_loss: 0.3328 - val_accuracy: 0.8855\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3330 - accuracy: 0.8826 - val_loss: 0.3308 - val_accuracy: 0.8855\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3310 - accuracy: 0.8828 - val_loss: 0.3289 - val_accuracy: 0.8858\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3291 - accuracy: 0.8829 - val_loss: 0.3270 - val_accuracy: 0.8858\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3272 - accuracy: 0.8832 - val_loss: 0.3252 - val_accuracy: 0.8858\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3254 - accuracy: 0.8833 - val_loss: 0.3234 - val_accuracy: 0.8863\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3236 - accuracy: 0.8834 - val_loss: 0.3217 - val_accuracy: 0.8864\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3219 - accuracy: 0.8834 - val_loss: 0.3200 - val_accuracy: 0.8864\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3202 - accuracy: 0.8833 - val_loss: 0.3183 - val_accuracy: 0.8863\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3185 - accuracy: 0.8831 - val_loss: 0.3167 - val_accuracy: 0.8861\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3169 - accuracy: 0.8830 - val_loss: 0.3151 - val_accuracy: 0.8862\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3153 - accuracy: 0.8830 - val_loss: 0.3135 - val_accuracy: 0.8862\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3137 - accuracy: 0.8830 - val_loss: 0.3120 - val_accuracy: 0.8864\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3122 - accuracy: 0.8831 - val_loss: 0.3105 - val_accuracy: 0.8866\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3107 - accuracy: 0.8833 - val_loss: 0.3090 - val_accuracy: 0.8867\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3092 - accuracy: 0.8834 - val_loss: 0.3076 - val_accuracy: 0.8873\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3078 - accuracy: 0.8836 - val_loss: 0.3061 - val_accuracy: 0.8880\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3064 - accuracy: 0.8840 - val_loss: 0.3048 - val_accuracy: 0.8882\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3050 - accuracy: 0.8844 - val_loss: 0.3034 - val_accuracy: 0.8884\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3037 - accuracy: 0.8846 - val_loss: 0.3021 - val_accuracy: 0.8887\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3023 - accuracy: 0.8851 - val_loss: 0.3008 - val_accuracy: 0.8893\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.3010 - accuracy: 0.8858 - val_loss: 0.2995 - val_accuracy: 0.8898\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.2997 - accuracy: 0.8895 - val_loss: 0.2982 - val_accuracy: 0.8943\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2985 - accuracy: 0.8925 - val_loss: 0.2970 - val_accuracy: 0.8996\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2972 - accuracy: 0.8962 - val_loss: 0.2958 - val_accuracy: 0.8997\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.2960 - accuracy: 0.8965 - val_loss: 0.2946 - val_accuracy: 0.8999\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2948 - accuracy: 0.8967 - val_loss: 0.2934 - val_accuracy: 0.9001\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.2936 - accuracy: 0.8970 - val_loss: 0.2923 - val_accuracy: 0.9002\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2925 - accuracy: 0.8972 - val_loss: 0.2912 - val_accuracy: 0.9003\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.2914 - accuracy: 0.8975 - val_loss: 0.2901 - val_accuracy: 0.9006\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2903 - accuracy: 0.8978 - val_loss: 0.2890 - val_accuracy: 0.9011\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2892 - accuracy: 0.8980 - val_loss: 0.2879 - val_accuracy: 0.9014\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2881 - accuracy: 0.8983 - val_loss: 0.2869 - val_accuracy: 0.9018\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2870 - accuracy: 0.8986 - val_loss: 0.2858 - val_accuracy: 0.9021\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.2860 - accuracy: 0.8987 - val_loss: 0.2848 - val_accuracy: 0.9027\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2850 - accuracy: 0.8993 - val_loss: 0.2838 - val_accuracy: 0.9029\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2840 - accuracy: 0.8997 - val_loss: 0.2828 - val_accuracy: 0.9032\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.2830 - accuracy: 0.9002 - val_loss: 0.2818 - val_accuracy: 0.9038\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2820 - accuracy: 0.9011 - val_loss: 0.2809 - val_accuracy: 0.9051\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2810 - accuracy: 0.9019 - val_loss: 0.2799 - val_accuracy: 0.9057\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.2801 - accuracy: 0.9027 - val_loss: 0.2790 - val_accuracy: 0.9059\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.2791 - accuracy: 0.9030 - val_loss: 0.2781 - val_accuracy: 0.9063\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2782 - accuracy: 0.9033 - val_loss: 0.2772 - val_accuracy: 0.9065\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2773 - accuracy: 0.9036 - val_loss: 0.2763 - val_accuracy: 0.9071\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2764 - accuracy: 0.9039 - val_loss: 0.2754 - val_accuracy: 0.9071\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2755 - accuracy: 0.9042 - val_loss: 0.2746 - val_accuracy: 0.9072\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2747 - accuracy: 0.9044 - val_loss: 0.2737 - val_accuracy: 0.9075\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2738 - accuracy: 0.9047 - val_loss: 0.2729 - val_accuracy: 0.9078\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2729 - accuracy: 0.9049 - val_loss: 0.2720 - val_accuracy: 0.9080\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2721 - accuracy: 0.9052 - val_loss: 0.2712 - val_accuracy: 0.9084\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2713 - accuracy: 0.9054 - val_loss: 0.2704 - val_accuracy: 0.9090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining loss function, optimizer, metrics and then compiling model\n",
        "mlp1.compile(loss='binary_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
        "\n",
        "# predicting target attribute on testing dataset\n",
        "test_results = mlp1.evaluate(X_test, y_test, verbose=1)\n",
        "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}')\n",
        "\n",
        "y_pred = mlp1.predict(X_test).ravel()\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "# auc = auc(fpr, tpr)\n",
        "\n",
        "pred = mlp1.predict(X_test)\n",
        "y_classes = (mlp.predict(X_test)>0.5).astype('int32')\n",
        "\n",
        "print(\"Recall Score - \",recall_score(y_test,y_classes))\n",
        "print(\"F1 Score - \",f1_score(y_test,y_classes))\n",
        "print(\"Precision Score - \",precision_score(y_test,y_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4MXegUIhpO_",
        "outputId": "baf64555-bb0e-4750-fb55-5bdab0e07409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "985/985 [==============================] - 2s 2ms/step - loss: 0.2712 - accuracy: 0.9045\n",
            "Test results - Loss: 0.2711753249168396 - Accuracy: 90.44897556304932\n",
            "Recall Score -  0.9873017765589603\n",
            "F1 Score -  0.9817125581671065\n",
            "Precision Score -  0.9761862658414383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mi2kgP_Tq_Lt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gi4PJFSYq_Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSPROP"
      ],
      "metadata": {
        "id": "9llQUA7qrJUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = bin_data.iloc[:,0:93].values # dataset excluding target attribute (encoded, one-hot-encoded,original)\n",
        "Y1 = bin_data[['intrusion']].values # target attribute\n",
        "\n",
        "# X = bin_data.iloc[:,0:93].to_numpy() # dataset excluding target attribute (encoded, one-hot-encoded,original)\n",
        "# Y = bin_data['intrusion'] # target attribute\n",
        "\n",
        "# splitting the dataset 75% for training and 25% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X1,Y1, test_size=0.25, random_state=42)\n",
        "\n",
        "mlp1 = Sequential() # creating model\n",
        "\n",
        "# adding input layer and first layer with 50 neurons\n",
        "mlp1.add(Dense(units=50, input_dim=X_train.shape[1], activation='relu'))\n",
        "# output layer with sigmoid activation\n",
        "mlp1.add(Dense(units=1,activation='sigmoid'))\n",
        "\n",
        "# defining loss function, optimizer, metrics and then compiling model\n",
        "mlp1.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "# summary of model layers\n",
        "mlp1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o43xA2aPq_Gk",
        "outputId": "e21b53ef-0e46-4a5e-b143-ad2cdcdb63ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_28 (Dense)            (None, 50)                4700      \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,751\n",
            "Trainable params: 4,751\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model on training dataset\n",
        "history = mlp1.fit(X_train, y_train, epochs=100, batch_size=5000,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdf5FMLJq_EF",
        "outputId": "8045451b-f3d4-452d-b86d-c23ea6c139d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 15ms/step - loss: 0.4837 - accuracy: 0.8578 - val_loss: 0.3576 - val_accuracy: 0.9094\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.3074 - accuracy: 0.9143 - val_loss: 0.2613 - val_accuracy: 0.9201\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.2353 - accuracy: 0.9206 - val_loss: 0.2112 - val_accuracy: 0.9237\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1949 - accuracy: 0.9254 - val_loss: 0.1801 - val_accuracy: 0.9379\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1680 - accuracy: 0.9505 - val_loss: 0.1583 - val_accuracy: 0.9576\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1482 - accuracy: 0.9597 - val_loss: 0.1415 - val_accuracy: 0.9620\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.1328 - accuracy: 0.9632 - val_loss: 0.1280 - val_accuracy: 0.9634\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.1207 - accuracy: 0.9644 - val_loss: 0.1174 - val_accuracy: 0.9655\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.1113 - accuracy: 0.9663 - val_loss: 0.1089 - val_accuracy: 0.9674\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.1037 - accuracy: 0.9689 - val_loss: 0.1021 - val_accuracy: 0.9696\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0975 - accuracy: 0.9704 - val_loss: 0.0965 - val_accuracy: 0.9700\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0925 - accuracy: 0.9709 - val_loss: 0.0921 - val_accuracy: 0.9700\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0885 - accuracy: 0.9710 - val_loss: 0.0883 - val_accuracy: 0.9704\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0852 - accuracy: 0.9711 - val_loss: 0.0853 - val_accuracy: 0.9697\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0824 - accuracy: 0.9711 - val_loss: 0.0824 - val_accuracy: 0.9699\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0801 - accuracy: 0.9713 - val_loss: 0.0804 - val_accuracy: 0.9702\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0780 - accuracy: 0.9718 - val_loss: 0.0789 - val_accuracy: 0.9712\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0763 - accuracy: 0.9729 - val_loss: 0.0769 - val_accuracy: 0.9720\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0747 - accuracy: 0.9734 - val_loss: 0.0755 - val_accuracy: 0.9722\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0734 - accuracy: 0.9736 - val_loss: 0.0747 - val_accuracy: 0.9726\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0722 - accuracy: 0.9742 - val_loss: 0.0733 - val_accuracy: 0.9729\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0712 - accuracy: 0.9745 - val_loss: 0.0724 - val_accuracy: 0.9730\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0703 - accuracy: 0.9751 - val_loss: 0.0716 - val_accuracy: 0.9745\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0695 - accuracy: 0.9757 - val_loss: 0.0710 - val_accuracy: 0.9745\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0688 - accuracy: 0.9765 - val_loss: 0.0702 - val_accuracy: 0.9749\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0681 - accuracy: 0.9768 - val_loss: 0.0698 - val_accuracy: 0.9754\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0676 - accuracy: 0.9773 - val_loss: 0.0695 - val_accuracy: 0.9758\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0671 - accuracy: 0.9776 - val_loss: 0.0692 - val_accuracy: 0.9763\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0666 - accuracy: 0.9777 - val_loss: 0.0684 - val_accuracy: 0.9764\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0663 - accuracy: 0.9780 - val_loss: 0.0682 - val_accuracy: 0.9759\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0659 - accuracy: 0.9780 - val_loss: 0.0685 - val_accuracy: 0.9760\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0656 - accuracy: 0.9780 - val_loss: 0.0675 - val_accuracy: 0.9765\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0652 - accuracy: 0.9782 - val_loss: 0.0672 - val_accuracy: 0.9766\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0648 - accuracy: 0.9782 - val_loss: 0.0669 - val_accuracy: 0.9763\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0646 - accuracy: 0.9783 - val_loss: 0.0670 - val_accuracy: 0.9766\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0643 - accuracy: 0.9782 - val_loss: 0.0665 - val_accuracy: 0.9767\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0640 - accuracy: 0.9784 - val_loss: 0.0663 - val_accuracy: 0.9767\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0637 - accuracy: 0.9784 - val_loss: 0.0662 - val_accuracy: 0.9767\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0635 - accuracy: 0.9785 - val_loss: 0.0667 - val_accuracy: 0.9767\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0633 - accuracy: 0.9786 - val_loss: 0.0659 - val_accuracy: 0.9766\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0631 - accuracy: 0.9787 - val_loss: 0.0661 - val_accuracy: 0.9768\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0630 - accuracy: 0.9787 - val_loss: 0.0653 - val_accuracy: 0.9765\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0628 - accuracy: 0.9786 - val_loss: 0.0654 - val_accuracy: 0.9767\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0626 - accuracy: 0.9787 - val_loss: 0.0652 - val_accuracy: 0.9767\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0624 - accuracy: 0.9788 - val_loss: 0.0649 - val_accuracy: 0.9765\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0623 - accuracy: 0.9788 - val_loss: 0.0652 - val_accuracy: 0.9767\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0621 - accuracy: 0.9788 - val_loss: 0.0648 - val_accuracy: 0.9766\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0620 - accuracy: 0.9789 - val_loss: 0.0645 - val_accuracy: 0.9767\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0618 - accuracy: 0.9790 - val_loss: 0.0645 - val_accuracy: 0.9763\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0616 - accuracy: 0.9789 - val_loss: 0.0645 - val_accuracy: 0.9767\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0615 - accuracy: 0.9791 - val_loss: 0.0645 - val_accuracy: 0.9772\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0614 - accuracy: 0.9791 - val_loss: 0.0643 - val_accuracy: 0.9767\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0612 - accuracy: 0.9791 - val_loss: 0.0642 - val_accuracy: 0.9772\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0611 - accuracy: 0.9792 - val_loss: 0.0642 - val_accuracy: 0.9771\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0610 - accuracy: 0.9793 - val_loss: 0.0640 - val_accuracy: 0.9777\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0609 - accuracy: 0.9796 - val_loss: 0.0640 - val_accuracy: 0.9774\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0608 - accuracy: 0.9794 - val_loss: 0.0634 - val_accuracy: 0.9773\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0607 - accuracy: 0.9795 - val_loss: 0.0635 - val_accuracy: 0.9774\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0605 - accuracy: 0.9795 - val_loss: 0.0637 - val_accuracy: 0.9787\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0604 - accuracy: 0.9794 - val_loss: 0.0631 - val_accuracy: 0.9773\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0603 - accuracy: 0.9795 - val_loss: 0.0629 - val_accuracy: 0.9778\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0602 - accuracy: 0.9796 - val_loss: 0.0633 - val_accuracy: 0.9773\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0600 - accuracy: 0.9797 - val_loss: 0.0627 - val_accuracy: 0.9778\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0599 - accuracy: 0.9798 - val_loss: 0.0632 - val_accuracy: 0.9776\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0599 - accuracy: 0.9798 - val_loss: 0.0628 - val_accuracy: 0.9777\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0598 - accuracy: 0.9798 - val_loss: 0.0624 - val_accuracy: 0.9779\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0597 - accuracy: 0.9798 - val_loss: 0.0626 - val_accuracy: 0.9776\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 0.0596 - accuracy: 0.9798 - val_loss: 0.0625 - val_accuracy: 0.9785\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0595 - accuracy: 0.9799 - val_loss: 0.0623 - val_accuracy: 0.9783\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0593 - accuracy: 0.9801 - val_loss: 0.0624 - val_accuracy: 0.9781\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0593 - accuracy: 0.9801 - val_loss: 0.0625 - val_accuracy: 0.9785\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0593 - accuracy: 0.9801 - val_loss: 0.0623 - val_accuracy: 0.9787\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 10ms/step - loss: 0.0591 - accuracy: 0.9801 - val_loss: 0.0621 - val_accuracy: 0.9784\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0590 - accuracy: 0.9800 - val_loss: 0.0624 - val_accuracy: 0.9789\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0590 - accuracy: 0.9801 - val_loss: 0.0621 - val_accuracy: 0.9790\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0590 - accuracy: 0.9801 - val_loss: 0.0619 - val_accuracy: 0.9787\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0588 - accuracy: 0.9802 - val_loss: 0.0616 - val_accuracy: 0.9784\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0587 - accuracy: 0.9802 - val_loss: 0.0614 - val_accuracy: 0.9780\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0586 - accuracy: 0.9802 - val_loss: 0.0616 - val_accuracy: 0.9786\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0586 - accuracy: 0.9803 - val_loss: 0.0612 - val_accuracy: 0.9781\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0584 - accuracy: 0.9802 - val_loss: 0.0617 - val_accuracy: 0.9791\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0585 - accuracy: 0.9803 - val_loss: 0.0614 - val_accuracy: 0.9788\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0583 - accuracy: 0.9803 - val_loss: 0.0610 - val_accuracy: 0.9782\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0582 - accuracy: 0.9803 - val_loss: 0.0611 - val_accuracy: 0.9788\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0582 - accuracy: 0.9804 - val_loss: 0.0612 - val_accuracy: 0.9790\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0580 - accuracy: 0.9804 - val_loss: 0.0614 - val_accuracy: 0.9784\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0580 - accuracy: 0.9803 - val_loss: 0.0609 - val_accuracy: 0.9787\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0579 - accuracy: 0.9803 - val_loss: 0.0612 - val_accuracy: 0.9781\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0578 - accuracy: 0.9802 - val_loss: 0.0607 - val_accuracy: 0.9784\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0577 - accuracy: 0.9804 - val_loss: 0.0608 - val_accuracy: 0.9790\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0576 - accuracy: 0.9804 - val_loss: 0.0607 - val_accuracy: 0.9789\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0576 - accuracy: 0.9805 - val_loss: 0.0615 - val_accuracy: 0.9790\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0576 - accuracy: 0.9805 - val_loss: 0.0603 - val_accuracy: 0.9795\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0575 - accuracy: 0.9807 - val_loss: 0.0604 - val_accuracy: 0.9790\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0574 - accuracy: 0.9804 - val_loss: 0.0602 - val_accuracy: 0.9793\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0573 - accuracy: 0.9807 - val_loss: 0.0604 - val_accuracy: 0.9792\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0573 - accuracy: 0.9807 - val_loss: 0.0603 - val_accuracy: 0.9789\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0572 - accuracy: 0.9807 - val_loss: 0.0606 - val_accuracy: 0.9791\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 0.0572 - accuracy: 0.9807 - val_loss: 0.0602 - val_accuracy: 0.9794\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 8ms/step - loss: 0.0571 - accuracy: 0.9806 - val_loss: 0.0601 - val_accuracy: 0.9785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining loss function, optimizer, metrics and then compiling model\n",
        "mlp1.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "# predicting target attribute on testing dataset\n",
        "test_results = mlp1.evaluate(X_test, y_test, verbose=1)\n",
        "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}')\n",
        "\n",
        "y_pred = mlp1.predict(X_test).ravel()\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "# auc = auc(fpr, tpr)\n",
        "\n",
        "pred = mlp1.predict(X_test)\n",
        "y_classes = (mlp.predict(X_test)>0.5).astype('int32')\n",
        "\n",
        "print(\"Recall Score - \",recall_score(y_test,y_classes))\n",
        "print(\"F1 Score - \",f1_score(y_test,y_classes))\n",
        "print(\"Precision Score - \",precision_score(y_test,y_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMrBEZVgq--c",
        "outputId": "b9b23438-7d6e-4dd5-8612-252d9946db70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "985/985 [==============================] - 1s 1ms/step - loss: 0.0594 - accuracy: 0.9799\n",
            "Test results - Loss: 0.059357259422540665 - Accuracy: 97.99009561538696\n",
            "Recall Score -  0.9873017765589603\n",
            "F1 Score -  0.9817125581671065\n",
            "Precision Score -  0.9761862658414383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WEhDYnXRq-7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xxscJ8MhhpMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "__qQjWWMhpHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NnJs2nclho9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot feature importance manually\n",
        "from numpy import loadtxt\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot\n",
        "# # load data\n",
        "# dataset = loadtxt('pima-indians-diabetes.csv', delimiter=\",\")\n",
        "# # split data into X and y\n",
        "# X = dataset[:,0:8]\n",
        "# y = dataset[:,8]\n",
        "# fit model no training data\n",
        "model = XGBClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "# feature importance\n",
        "print(model.feature_importances_)\n",
        "\n",
        "pyplot.rcParams[\"figure.figsize\"] = (10,6)\n",
        "plot_importance(model)\n",
        "pyplot.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "id": "hqBRASg9kG3R",
        "outputId": "908ba806-05c2-4509-d8a2-5ef3544a7363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.2488088e-02 4.7561823e-04 2.0278534e-03 6.1635249e-03 3.6414491e-03\n",
            " 6.8162056e-03 2.4774628e-02 3.3723697e-02 1.3769719e-01 7.8788646e-02\n",
            " 2.3329605e-03 1.8829515e-03 4.2538485e-03 1.9715035e-03 0.0000000e+00\n",
            " 0.0000000e+00 9.6663868e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.4208863e-03\n",
            " 0.0000000e+00 5.3251470e-03 6.4531197e-03 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 6.5191691e-03 8.6629782e-03 9.1445149e-04 0.0000000e+00\n",
            " 0.0000000e+00 1.0640386e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 1.4602155e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 4.7824319e-02 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 7.7188080e-03 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 0.0000000e+00 5.3234729e-03 0.0000000e+00 0.0000000e+00\n",
            " 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.4947509e-03 1.2758619e-03\n",
            " 0.0000000e+00 4.7588078e-03 6.3343986e-04 4.0331283e-03 0.0000000e+00\n",
            " 0.0000000e+00 5.6811261e-01 0.0000000e+00]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "selection = SelectFromModel(model, prefit=True)\n",
        "select_X_train = selection.transform(X_train)"
      ],
      "metadata": {
        "id": "v5kbDgYkkG0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(select_X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MquDiBI7mv0A",
        "outputId": "981f72d4-31c3-43bc-b1d6-1071dd56e6ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G-xcD1DzmvxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x5SipSAmmvuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W4jPfDjrmvrx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}