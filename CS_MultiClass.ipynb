{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS-MultiClass.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNr1Ol8dEfJtaBvsn+fHhZl"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7anDI0j2RG7j",
        "outputId": "c349c6cb-ca45-4b1d-ef3b-1027f1abed54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import pickle # saving and loading trained model\n",
        "from os import path\n",
        "\n",
        "# importing required libraries for normalizing data\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# importing library for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# importing library for support vector machine classifier\n",
        "from sklearn.svm import SVC\n",
        "# importing library for K-neares-neighbor classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "# importing library for Linear Discriminant Analysis Model\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "# importing library for Quadratic Discriminant Analysis Model\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score # for calculating accuracy of model\n",
        "from sklearn.model_selection import train_test_split # for splitting the dataset for training and testing\n",
        "from sklearn.metrics import classification_report # for generating a classification report of model\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "from keras.layers import Dense # importing dense layer\n",
        "from keras.models import Sequential #importing Sequential layer\n",
        "from keras.models import model_from_json # saving and loading trained model\n",
        "\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "\n",
        "# representation of model layers\n",
        "from keras.utils.vis_utils import plot_model"
      ],
      "metadata": {
        "id": "1bZKdn41sqP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
        "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
        "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
        "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
        "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
        "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
        "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
        "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
        "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
        "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\",\"difficulty_level\"]"
      ],
      "metadata": {
        "id": "MuBHMAw4ssdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/cyberData/KDDTrain+.txt',header=None, names=col_names)\n",
        "data_t = pd.read_table(\"/content/drive/MyDrive/cyberData/KDDTest+.txt\",header=None, names=col_names)"
      ],
      "metadata": {
        "id": "TC93MT7lic6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "XcGADzivim3p",
        "outputId": "98db3c85-16c6-4fdc-d476-b8f99866b856"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        duration protocol_type   service flag  src_bytes  dst_bytes  land  \\\n",
              "0              0           tcp  ftp_data   SF        491          0     0   \n",
              "1              0           udp     other   SF        146          0     0   \n",
              "2              0           tcp   private   S0          0          0     0   \n",
              "3              0           tcp      http   SF        232       8153     0   \n",
              "4              0           tcp      http   SF        199        420     0   \n",
              "...          ...           ...       ...  ...        ...        ...   ...   \n",
              "125968         0           tcp   private   S0          0          0     0   \n",
              "125969         8           udp   private   SF        105        145     0   \n",
              "125970         0           tcp      smtp   SF       2231        384     0   \n",
              "125971         0           tcp    klogin   S0          0          0     0   \n",
              "125972         0           tcp  ftp_data   SF        151          0     0   \n",
              "\n",
              "        wrong_fragment  urgent  hot  ...  dst_host_same_srv_rate  \\\n",
              "0                    0       0    0  ...                    0.17   \n",
              "1                    0       0    0  ...                    0.00   \n",
              "2                    0       0    0  ...                    0.10   \n",
              "3                    0       0    0  ...                    1.00   \n",
              "4                    0       0    0  ...                    1.00   \n",
              "...                ...     ...  ...  ...                     ...   \n",
              "125968               0       0    0  ...                    0.10   \n",
              "125969               0       0    0  ...                    0.96   \n",
              "125970               0       0    0  ...                    0.12   \n",
              "125971               0       0    0  ...                    0.03   \n",
              "125972               0       0    0  ...                    0.30   \n",
              "\n",
              "        dst_host_diff_srv_rate  dst_host_same_src_port_rate  \\\n",
              "0                         0.03                         0.17   \n",
              "1                         0.60                         0.88   \n",
              "2                         0.05                         0.00   \n",
              "3                         0.00                         0.03   \n",
              "4                         0.00                         0.00   \n",
              "...                        ...                          ...   \n",
              "125968                    0.06                         0.00   \n",
              "125969                    0.01                         0.01   \n",
              "125970                    0.06                         0.00   \n",
              "125971                    0.05                         0.00   \n",
              "125972                    0.03                         0.30   \n",
              "\n",
              "        dst_host_srv_diff_host_rate  dst_host_serror_rate  \\\n",
              "0                              0.00                  0.00   \n",
              "1                              0.00                  0.00   \n",
              "2                              0.00                  1.00   \n",
              "3                              0.04                  0.03   \n",
              "4                              0.00                  0.00   \n",
              "...                             ...                   ...   \n",
              "125968                         0.00                  1.00   \n",
              "125969                         0.00                  0.00   \n",
              "125970                         0.00                  0.72   \n",
              "125971                         0.00                  1.00   \n",
              "125972                         0.00                  0.00   \n",
              "\n",
              "        dst_host_srv_serror_rate  dst_host_rerror_rate  \\\n",
              "0                           0.00                  0.05   \n",
              "1                           0.00                  0.00   \n",
              "2                           1.00                  0.00   \n",
              "3                           0.01                  0.00   \n",
              "4                           0.00                  0.00   \n",
              "...                          ...                   ...   \n",
              "125968                      1.00                  0.00   \n",
              "125969                      0.00                  0.00   \n",
              "125970                      0.00                  0.01   \n",
              "125971                      1.00                  0.00   \n",
              "125972                      0.00                  0.00   \n",
              "\n",
              "        dst_host_srv_rerror_rate    label  difficulty_level  \n",
              "0                           0.00   normal                20  \n",
              "1                           0.00   normal                15  \n",
              "2                           0.00  neptune                19  \n",
              "3                           0.01   normal                21  \n",
              "4                           0.00   normal                21  \n",
              "...                          ...      ...               ...  \n",
              "125968                      0.00  neptune                20  \n",
              "125969                      0.00   normal                21  \n",
              "125970                      0.00   normal                18  \n",
              "125971                      0.00  neptune                20  \n",
              "125972                      0.00   normal                21  \n",
              "\n",
              "[125973 rows x 43 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-464035e3-8859-4da6-b3ba-0e10842f759e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>duration</th>\n",
              "      <th>protocol_type</th>\n",
              "      <th>service</th>\n",
              "      <th>flag</th>\n",
              "      <th>src_bytes</th>\n",
              "      <th>dst_bytes</th>\n",
              "      <th>land</th>\n",
              "      <th>wrong_fragment</th>\n",
              "      <th>urgent</th>\n",
              "      <th>hot</th>\n",
              "      <th>...</th>\n",
              "      <th>dst_host_same_srv_rate</th>\n",
              "      <th>dst_host_diff_srv_rate</th>\n",
              "      <th>dst_host_same_src_port_rate</th>\n",
              "      <th>dst_host_srv_diff_host_rate</th>\n",
              "      <th>dst_host_serror_rate</th>\n",
              "      <th>dst_host_srv_serror_rate</th>\n",
              "      <th>dst_host_rerror_rate</th>\n",
              "      <th>dst_host_srv_rerror_rate</th>\n",
              "      <th>label</th>\n",
              "      <th>difficulty_level</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>ftp_data</td>\n",
              "      <td>SF</td>\n",
              "      <td>491</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>udp</td>\n",
              "      <td>other</td>\n",
              "      <td>SF</td>\n",
              "      <td>146</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.60</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>private</td>\n",
              "      <td>S0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>neptune</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>232</td>\n",
              "      <td>8153</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.04</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>normal</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>http</td>\n",
              "      <td>SF</td>\n",
              "      <td>199</td>\n",
              "      <td>420</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125968</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>private</td>\n",
              "      <td>S0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>neptune</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125969</th>\n",
              "      <td>8</td>\n",
              "      <td>udp</td>\n",
              "      <td>private</td>\n",
              "      <td>SF</td>\n",
              "      <td>105</td>\n",
              "      <td>145</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125970</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>smtp</td>\n",
              "      <td>SF</td>\n",
              "      <td>2231</td>\n",
              "      <td>384</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.72</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125971</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>klogin</td>\n",
              "      <td>S0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>neptune</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125972</th>\n",
              "      <td>0</td>\n",
              "      <td>tcp</td>\n",
              "      <td>ftp_data</td>\n",
              "      <td>SF</td>\n",
              "      <td>151</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>normal</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125973 rows × 43 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-464035e3-8859-4da6-b3ba-0e10842f759e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-464035e3-8859-4da6-b3ba-0e10842f759e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-464035e3-8859-4da6-b3ba-0e10842f759e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove attribute 'difficulty_level'\n",
        "data.drop(['difficulty_level'],axis=1,inplace=True)\n",
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VycFUFSJiscx",
        "outputId": "88429cd1-2027-4707-f508-019cca9bdd62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(125973, 42)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove attribute 'difficulty_level'\n",
        "data_t.drop(['difficulty_level'],axis=1,inplace=True)\n",
        "data_t.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz5fTrfUtc0y",
        "outputId": "03a89713-8333-44cf-ff2c-fabb80c9f135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22544, 42)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# number of attack labels \n",
        "data['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iA_OzN4Mivuy",
        "outputId": "c0eb9778-8a40-4c0a-8373-30050c2fa7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "normal             67343\n",
              "neptune            41214\n",
              "satan               3633\n",
              "ipsweep             3599\n",
              "portsweep           2931\n",
              "smurf               2646\n",
              "nmap                1493\n",
              "back                 956\n",
              "teardrop             892\n",
              "warezclient          890\n",
              "pod                  201\n",
              "guess_passwd          53\n",
              "buffer_overflow       30\n",
              "warezmaster           20\n",
              "land                  18\n",
              "imap                  11\n",
              "rootkit               10\n",
              "loadmodule             9\n",
              "ftp_write              8\n",
              "multihop               7\n",
              "phf                    4\n",
              "perl                   3\n",
              "spy                    2\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# changing attack labels to their respective attack class\n",
        "def change_label(df):\n",
        "  df.label.replace(['apache2','back','land','neptune','mailbomb','pod','processtable','smurf','teardrop','udpstorm','worm'],'Dos',inplace=True)\n",
        "  df.label.replace(['ftp_write','guess_passwd','httptunnel','imap','multihop','named','phf','sendmail',\n",
        "       'snmpgetattack','snmpguess','spy','warezclient','warezmaster','xlock','xsnoop'],'R2L',inplace=True)\n",
        "  df.label.replace(['ipsweep','mscan','nmap','portsweep','saint','satan'],'Probe',inplace=True)\n",
        "  df.label.replace(['buffer_overflow','loadmodule','perl','ps','rootkit','sqlattack','xterm'],'U2R',inplace=True)"
      ],
      "metadata": {
        "id": "f6YGACjHi1B4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling change_label() function\n",
        "change_label(data)"
      ],
      "metadata": {
        "id": "0BAxhR3bi3Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "change_label(data_t)"
      ],
      "metadata": {
        "id": "psFvUXattjCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distribution of attack classes\n",
        "data.label.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUJ143f_i5zB",
        "outputId": "29f80edf-be95-46e7-cffc-ebd27bb23d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "normal    67343\n",
              "Dos       45927\n",
              "Probe     11656\n",
              "R2L         995\n",
              "U2R          52\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# selecting numeric attributes columns from data\n",
        "numeric_col = data.select_dtypes(include='number').columns\n",
        "\n",
        "# using standard scaler for normalizing\n",
        "std_scaler = StandardScaler()\n",
        "def normalization(df,col):\n",
        "  for i in col:\n",
        "    arr = df[i]\n",
        "    arr = np.array(arr)\n",
        "    df[i] = std_scaler.fit_transform(arr.reshape(len(arr),1))\n",
        "  return df\n",
        "\n",
        "# data before normalization\n",
        "data.head()\n",
        "\n",
        "# calling the normalization() function\n",
        "data = normalization(data.copy(),numeric_col)\n",
        "\n",
        "# data after normalization\n",
        "data.head()\n",
        "\n",
        "# selecting categorical data attributes\n",
        "cat_col = ['protocol_type','service','flag']\n",
        "\n",
        "# creating a dataframe with only categorical attributes\n",
        "categorical = data[cat_col]\n",
        "\n",
        "# one-hot-encoding categorical attributes using pandas.get_dummies() function\n",
        "categorical = pd.get_dummies(categorical,columns=cat_col)\n",
        "categorical.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "HI1-0zsStGtb",
        "outputId": "5468c160-a6b9-4771-f81f-988a1a6bcaea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   protocol_type_icmp  protocol_type_tcp  protocol_type_udp  service_IRC  \\\n",
              "0                   0                  1                  0            0   \n",
              "1                   0                  0                  1            0   \n",
              "2                   0                  1                  0            0   \n",
              "3                   0                  1                  0            0   \n",
              "4                   0                  1                  0            0   \n",
              "\n",
              "   service_X11  service_Z39_50  service_aol  service_auth  service_bgp  \\\n",
              "0            0               0            0             0            0   \n",
              "1            0               0            0             0            0   \n",
              "2            0               0            0             0            0   \n",
              "3            0               0            0             0            0   \n",
              "4            0               0            0             0            0   \n",
              "\n",
              "   service_courier  ...  flag_REJ  flag_RSTO  flag_RSTOS0  flag_RSTR  flag_S0  \\\n",
              "0                0  ...         0          0            0          0        0   \n",
              "1                0  ...         0          0            0          0        0   \n",
              "2                0  ...         0          0            0          0        1   \n",
              "3                0  ...         0          0            0          0        0   \n",
              "4                0  ...         0          0            0          0        0   \n",
              "\n",
              "   flag_S1  flag_S2  flag_S3  flag_SF  flag_SH  \n",
              "0        0        0        0        1        0  \n",
              "1        0        0        0        1        0  \n",
              "2        0        0        0        0        0  \n",
              "3        0        0        0        1        0  \n",
              "4        0        0        0        1        0  \n",
              "\n",
              "[5 rows x 84 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-42237ab9-9bec-40e8-9127-e0519e7b722c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>protocol_type_icmp</th>\n",
              "      <th>protocol_type_tcp</th>\n",
              "      <th>protocol_type_udp</th>\n",
              "      <th>service_IRC</th>\n",
              "      <th>service_X11</th>\n",
              "      <th>service_Z39_50</th>\n",
              "      <th>service_aol</th>\n",
              "      <th>service_auth</th>\n",
              "      <th>service_bgp</th>\n",
              "      <th>service_courier</th>\n",
              "      <th>...</th>\n",
              "      <th>flag_REJ</th>\n",
              "      <th>flag_RSTO</th>\n",
              "      <th>flag_RSTOS0</th>\n",
              "      <th>flag_RSTR</th>\n",
              "      <th>flag_S0</th>\n",
              "      <th>flag_S1</th>\n",
              "      <th>flag_S2</th>\n",
              "      <th>flag_S3</th>\n",
              "      <th>flag_SF</th>\n",
              "      <th>flag_SH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 84 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42237ab9-9bec-40e8-9127-e0519e7b722c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-42237ab9-9bec-40e8-9127-e0519e7b722c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-42237ab9-9bec-40e8-9127-e0519e7b722c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a dataframe with multi-class labels (Dos,Probe,R2L,U2R,normal)\n",
        "multi_data = data.copy()\n",
        "multi_label = pd.DataFrame(multi_data.label)\n",
        "\n",
        "# label encoding (0,1,2,3,4) multi-class labels (Dos,normal,Probe,R2L,U2R)\n",
        "le2 = preprocessing.LabelEncoder()\n",
        "enc_label = multi_label.apply(le2.fit_transform)\n",
        "multi_data['intrusion'] = enc_label\n",
        "\n",
        "# one-hot-encoding attack label\n",
        "multi_data = pd.get_dummies(multi_data,columns=['label'],prefix=\"\",prefix_sep=\"\") \n",
        "multi_data['label'] = multi_label\n",
        "multi_data\n",
        "\n",
        "# pie chart distribution of multi-class labels\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(multi_data.label.value_counts(),labels=multi_data.label.unique(),autopct='%0.2f%%')\n",
        "plt.title('Pie chart distribution of multi-class labels')\n",
        "plt.legend()\n",
        "# plt.savefig('plots/Pie_chart_multi.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "OB_061bStbMb",
        "outputId": "e7fac9f9-8171-4d05-e5c2-785b820de844"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a dataframe with only numeric attributes of multi-class dataset and encoded label attribute \n",
        "numeric_multi = multi_data[numeric_col]\n",
        "numeric_multi['intrusion'] = multi_data['intrusion']\n",
        "\n",
        "# finding the attributes which have more than 0.5 correlation with encoded attack label attribute \n",
        "corr = numeric_multi.corr()\n",
        "corr_y = abs(corr['intrusion'])\n",
        "highest_corr = corr_y[corr_y >0.5]\n",
        "highest_corr.sort_values(ascending=True)\n",
        "\n",
        "# selecting attributes found by using pearson correlation coefficient\n",
        "numeric_multi = multi_data[['count','logged_in','srv_serror_rate','serror_rate','dst_host_serror_rate',\n",
        "                        'dst_host_same_srv_rate','dst_host_srv_serror_rate','dst_host_srv_count','same_srv_rate']]\n",
        "\n",
        "# joining the selected attribute with the one-hot-encoded categorical dataframe\n",
        "numeric_multi = numeric_multi.join(categorical)\n",
        "# then joining encoded, one-hot-encoded, and original attack label attribute\n",
        "multi_data = numeric_multi.join(multi_data[['intrusion','Dos','Probe','R2L','U2R','normal','label']])\n",
        "\n",
        "multi_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "TiTGLPwiuGbs",
        "outputId": "c003e65c-b626-4dda-c5ea-12cbf5bfc2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           count  logged_in  srv_serror_rate  serror_rate  \\\n",
              "0      -0.717045  -0.809262        -0.631929    -0.637209   \n",
              "1      -0.620982  -0.809262        -0.631929    -0.637209   \n",
              "2       0.339648  -0.809262         1.605104     1.602664   \n",
              "3      -0.690846   1.235694        -0.184522    -0.189235   \n",
              "4      -0.472521   1.235694        -0.631929    -0.637209   \n",
              "...          ...        ...              ...          ...   \n",
              "125968  0.872361  -0.809262         1.605104     1.602664   \n",
              "125969 -0.717045  -0.809262        -0.631929    -0.637209   \n",
              "125970 -0.725778   1.235694        -0.631929    -0.637209   \n",
              "125971  0.523041  -0.809262         1.605104     1.602664   \n",
              "125972 -0.725778   1.235694        -0.631929    -0.637209   \n",
              "\n",
              "        dst_host_serror_rate  dst_host_same_srv_rate  \\\n",
              "0                  -0.639532               -0.782367   \n",
              "1                  -0.639532               -1.161030   \n",
              "2                   1.608759               -0.938287   \n",
              "3                  -0.572083                1.066401   \n",
              "4                  -0.639532                1.066401   \n",
              "...                      ...                     ...   \n",
              "125968              1.608759               -0.938287   \n",
              "125969             -0.639532                0.977304   \n",
              "125970              0.979238               -0.893738   \n",
              "125971              1.608759               -1.094207   \n",
              "125972             -0.639532               -0.492801   \n",
              "\n",
              "        dst_host_srv_serror_rate  dst_host_srv_count  same_srv_rate  \\\n",
              "0                      -0.624871           -0.818890       0.771283   \n",
              "1                      -0.624871           -1.035688      -1.321428   \n",
              "2                       1.618955           -0.809857      -1.389669   \n",
              "3                      -0.602433            1.258754       0.771283   \n",
              "4                      -0.624871            1.258754       0.771283   \n",
              "...                          ...                 ...            ...   \n",
              "125968                  1.618955           -0.818890      -1.184947   \n",
              "125969                 -0.624871            1.159389       0.771283   \n",
              "125970                 -0.624871           -0.773724       0.771283   \n",
              "125971                  1.618955           -0.972455      -1.366922   \n",
              "125972                 -0.624871           -0.349162       0.771283   \n",
              "\n",
              "        protocol_type_icmp  ...  flag_S3  flag_SF  flag_SH  intrusion  Dos  \\\n",
              "0                        0  ...        0        1        0          4    0   \n",
              "1                        0  ...        0        1        0          4    0   \n",
              "2                        0  ...        0        0        0          0    1   \n",
              "3                        0  ...        0        1        0          4    0   \n",
              "4                        0  ...        0        1        0          4    0   \n",
              "...                    ...  ...      ...      ...      ...        ...  ...   \n",
              "125968                   0  ...        0        0        0          0    1   \n",
              "125969                   0  ...        0        1        0          4    0   \n",
              "125970                   0  ...        0        1        0          4    0   \n",
              "125971                   0  ...        0        0        0          0    1   \n",
              "125972                   0  ...        0        1        0          4    0   \n",
              "\n",
              "        Probe  R2L  U2R  normal   label  \n",
              "0           0    0    0       1  normal  \n",
              "1           0    0    0       1  normal  \n",
              "2           0    0    0       0     Dos  \n",
              "3           0    0    0       1  normal  \n",
              "4           0    0    0       1  normal  \n",
              "...       ...  ...  ...     ...     ...  \n",
              "125968      0    0    0       0     Dos  \n",
              "125969      0    0    0       1  normal  \n",
              "125970      0    0    0       1  normal  \n",
              "125971      0    0    0       0     Dos  \n",
              "125972      0    0    0       1  normal  \n",
              "\n",
              "[125973 rows x 100 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bd0213bf-1f49-40bd-92b1-d8ab1af8e838\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>logged_in</th>\n",
              "      <th>srv_serror_rate</th>\n",
              "      <th>serror_rate</th>\n",
              "      <th>dst_host_serror_rate</th>\n",
              "      <th>dst_host_same_srv_rate</th>\n",
              "      <th>dst_host_srv_serror_rate</th>\n",
              "      <th>dst_host_srv_count</th>\n",
              "      <th>same_srv_rate</th>\n",
              "      <th>protocol_type_icmp</th>\n",
              "      <th>...</th>\n",
              "      <th>flag_S3</th>\n",
              "      <th>flag_SF</th>\n",
              "      <th>flag_SH</th>\n",
              "      <th>intrusion</th>\n",
              "      <th>Dos</th>\n",
              "      <th>Probe</th>\n",
              "      <th>R2L</th>\n",
              "      <th>U2R</th>\n",
              "      <th>normal</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.717045</td>\n",
              "      <td>-0.809262</td>\n",
              "      <td>-0.631929</td>\n",
              "      <td>-0.637209</td>\n",
              "      <td>-0.639532</td>\n",
              "      <td>-0.782367</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>-0.818890</td>\n",
              "      <td>0.771283</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.620982</td>\n",
              "      <td>-0.809262</td>\n",
              "      <td>-0.631929</td>\n",
              "      <td>-0.637209</td>\n",
              "      <td>-0.639532</td>\n",
              "      <td>-1.161030</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>-1.035688</td>\n",
              "      <td>-1.321428</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.339648</td>\n",
              "      <td>-0.809262</td>\n",
              "      <td>1.605104</td>\n",
              "      <td>1.602664</td>\n",
              "      <td>1.608759</td>\n",
              "      <td>-0.938287</td>\n",
              "      <td>1.618955</td>\n",
              "      <td>-0.809857</td>\n",
              "      <td>-1.389669</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Dos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.690846</td>\n",
              "      <td>1.235694</td>\n",
              "      <td>-0.184522</td>\n",
              "      <td>-0.189235</td>\n",
              "      <td>-0.572083</td>\n",
              "      <td>1.066401</td>\n",
              "      <td>-0.602433</td>\n",
              "      <td>1.258754</td>\n",
              "      <td>0.771283</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.472521</td>\n",
              "      <td>1.235694</td>\n",
              "      <td>-0.631929</td>\n",
              "      <td>-0.637209</td>\n",
              "      <td>-0.639532</td>\n",
              "      <td>1.066401</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>1.258754</td>\n",
              "      <td>0.771283</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125968</th>\n",
              "      <td>0.872361</td>\n",
              "      <td>-0.809262</td>\n",
              "      <td>1.605104</td>\n",
              "      <td>1.602664</td>\n",
              "      <td>1.608759</td>\n",
              "      <td>-0.938287</td>\n",
              "      <td>1.618955</td>\n",
              "      <td>-0.818890</td>\n",
              "      <td>-1.184947</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Dos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125969</th>\n",
              "      <td>-0.717045</td>\n",
              "      <td>-0.809262</td>\n",
              "      <td>-0.631929</td>\n",
              "      <td>-0.637209</td>\n",
              "      <td>-0.639532</td>\n",
              "      <td>0.977304</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>1.159389</td>\n",
              "      <td>0.771283</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125970</th>\n",
              "      <td>-0.725778</td>\n",
              "      <td>1.235694</td>\n",
              "      <td>-0.631929</td>\n",
              "      <td>-0.637209</td>\n",
              "      <td>0.979238</td>\n",
              "      <td>-0.893738</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>-0.773724</td>\n",
              "      <td>0.771283</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125971</th>\n",
              "      <td>0.523041</td>\n",
              "      <td>-0.809262</td>\n",
              "      <td>1.605104</td>\n",
              "      <td>1.602664</td>\n",
              "      <td>1.608759</td>\n",
              "      <td>-1.094207</td>\n",
              "      <td>1.618955</td>\n",
              "      <td>-0.972455</td>\n",
              "      <td>-1.366922</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Dos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125972</th>\n",
              "      <td>-0.725778</td>\n",
              "      <td>1.235694</td>\n",
              "      <td>-0.631929</td>\n",
              "      <td>-0.637209</td>\n",
              "      <td>-0.639532</td>\n",
              "      <td>-0.492801</td>\n",
              "      <td>-0.624871</td>\n",
              "      <td>-0.349162</td>\n",
              "      <td>0.771283</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>125973 rows × 100 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd0213bf-1f49-40bd-92b1-d8ab1af8e838')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bd0213bf-1f49-40bd-92b1-d8ab1af8e838 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bd0213bf-1f49-40bd-92b1-d8ab1af8e838');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jLCiA3wauiCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = multi_data.iloc[:,0:93].to_numpy() # dataset excluding target attribute (encoded, one-hot-encoded, original)\n",
        "Y = multi_data['intrusion'] # target attribute\n",
        "\n",
        "# splitting the dataset 75% for training and 25% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "PYwmeSIbuaDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSVM"
      ],
      "metadata": {
        "id": "rObP6LyFuiwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lsvm=SVC(kernel='linear',gamma='auto')\n",
        "lsvm.fit(X_train,y_train) # training model on training dataset\n",
        "\n",
        "y_pred=lsvm.predict(X_test) # predicting target attribute on testing dataset\n",
        "ac=accuracy_score(y_test, y_pred)*100  # calculating accuracy of predicted data\n",
        "print(\"LSVM-Classifier Multi-class Set-Accuracy is \", ac)\n",
        "\n",
        "# classification report\n",
        "print(classification_report(y_test, y_pred,target_names=le2.classes_))\n",
        "\n",
        "print(\"Mean Absolute Error - \" , metrics.mean_absolute_error(y_test, y_pred))\n",
        "print(\"Mean Squared Error - \" , metrics.mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error - \" , np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "print(\"R2 Score - \" , metrics.explained_variance_score(y_test, y_pred)*100)\n",
        "print(\"Accuracy - \",accuracy_score(y_test,y_pred)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx0DFCMNukbi",
        "outputId": "1870e32e-783a-4440-8880-96f01930a3da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSVM-Classifier Multi-class Set-Accuracy is  95.24988886772083\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Dos       0.95      0.96      0.96     11484\n",
            "       Probe       0.86      0.79      0.82      2947\n",
            "         R2L       0.61      0.60      0.61       274\n",
            "         U2R       0.00      0.00      0.00        15\n",
            "      normal       0.97      0.98      0.98     16774\n",
            "\n",
            "    accuracy                           0.95     31494\n",
            "   macro avg       0.68      0.67      0.67     31494\n",
            "weighted avg       0.95      0.95      0.95     31494\n",
            "\n",
            "Mean Absolute Error -  0.10125738235854448\n",
            "Mean Squared Error -  0.2831967993903601\n",
            "Root Mean Squared Error -  0.5321623806606026\n",
            "R2 Score -  92.1868366533396\n",
            "Accuracy -  95.24988886772083\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QLUk6dzWvHrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QSVM"
      ],
      "metadata": {
        "id": "j2Khhcq3vIPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qsvm=SVC(kernel='poly',gamma='auto')\n",
        "qsvm.fit(X_train,y_train) # training model on training dataset\n",
        "\n",
        "y_pred=qsvm.predict(X_test)  # predicting target attribute on testing dataset\n",
        "ac=accuracy_score(y_test, y_pred)*100  # calculating accuracy of predicted data\n",
        "print(\"QSVM-Classifier Multi-class Set-Accuracy is \", ac)\n",
        "\n",
        "# classification report\n",
        "print(classification_report(y_test, y_pred,target_names=le2.classes_))\n",
        "\n",
        "print(\"Mean Absolute Error - \" , metrics.mean_absolute_error(y_test, y_pred))\n",
        "print(\"Mean Squared Error - \" , metrics.mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error - \" , np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "print(\"R2 Score - \" , metrics.explained_variance_score(y_test, y_pred)*100)\n",
        "print(\"Accuracy - \",accuracy_score(y_test,y_pred)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5cEE6DWuwXE",
        "outputId": "107391a5-e99f-4812-9d95-20a3b9af64bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QSVM-Classifier Multi-class Set-Accuracy is  92.86213246967677\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Dos       0.96      0.94      0.95     11484\n",
            "       Probe       0.96      0.61      0.74      2947\n",
            "         R2L       0.00      0.00      0.00       274\n",
            "         U2R       0.00      0.00      0.00        15\n",
            "      normal       0.91      1.00      0.95     16774\n",
            "\n",
            "    accuracy                           0.93     31494\n",
            "   macro avg       0.56      0.51      0.53     31494\n",
            "weighted avg       0.92      0.93      0.92     31494\n",
            "\n",
            "Mean Absolute Error -  0.1998158379373849\n",
            "Mean Squared Error -  0.6449164920302279\n",
            "Root Mean Squared Error -  0.8030669287364709\n",
            "R2 Score -  82.88055324849559\n",
            "Accuracy -  92.86213246967677\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LDA"
      ],
      "metadata": {
        "id": "WUxkKU5fuuu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda = LinearDiscriminantAnalysis()\n",
        "lda.fit(X_train, y_train) # training model on training dataset\n",
        "\n",
        "y_pred = lda.predict(X_test)  # predicting target attribute on testing dataset\n",
        "ac=accuracy_score(y_test, y_pred)*100  # calculating accuracy of predicted data\n",
        "print(\"LDA-Classifier Set-Accuracy is \", ac)\n",
        "\n",
        "# classification report\n",
        "print(classification_report(y_test, y_pred,target_names=le2.classes_))\n",
        "\n",
        "print(\"Mean Absolute Error - \" , metrics.mean_absolute_error(y_test, y_pred))\n",
        "print(\"Mean Squared Error - \" , metrics.mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error - \" , np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "print(\"R2 Score - \" , metrics.explained_variance_score(y_test, y_pred)*100)\n",
        "print(\"Accuracy - \",accuracy_score(y_test,y_pred)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EtAI8-OvHpC",
        "outputId": "d7a8bace-135a-4b5e-afd8-2ee38eb43783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA-Classifier Set-Accuracy is  93.1923540991935\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Dos       0.94      0.96      0.95     11484\n",
            "       Probe       0.88      0.73      0.80      2947\n",
            "         R2L       0.37      0.89      0.52       274\n",
            "         U2R       0.03      0.47      0.06        15\n",
            "      normal       0.97      0.95      0.96     16774\n",
            "\n",
            "    accuracy                           0.93     31494\n",
            "   macro avg       0.64      0.80      0.66     31494\n",
            "weighted avg       0.94      0.93      0.94     31494\n",
            "\n",
            "Mean Absolute Error -  0.14380516923858513\n",
            "Mean Squared Error -  0.3957261700641392\n",
            "Root Mean Squared Error -  0.6290676991104687\n",
            "R2 Score -  89.08369120793829\n",
            "Accuracy -  93.1923540991935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "QDA"
      ],
      "metadata": {
        "id": "UjLSHYsOvTLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qda = QuadraticDiscriminantAnalysis()\n",
        "qda.fit(X_train, y_train) # training model on training dataset\n",
        "\n",
        "y_pred = qda.predict(X_test)  # predicting target attribute on testing dataset\n",
        "ac=accuracy_score(y_test, y_pred)*100  # calculating accuracy of predicted data\n",
        "print(\"QDA-Classifier Multi-class Set-Accuracy is \", ac)\n",
        "\n",
        "# classification report\n",
        "print(classification_report(y_test, y_pred,target_names=le2.classes_))\n",
        "\n",
        "print(\"Mean Absolute Error - \" , metrics.mean_absolute_error(y_test, y_pred))\n",
        "print(\"Mean Squared Error - \" , metrics.mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error - \" , np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
        "print(\"R2 Score - \" , metrics.explained_variance_score(y_test, y_pred)*100)\n",
        "print(\"Accuracy - \",accuracy_score(y_test,y_pred)*100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXpWaTFtvUQ6",
        "outputId": "81826025-748a-46e5-c9d6-597cabefe66a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QDA-Classifier Multi-class Set-Accuracy is  44.66882580809043\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         Dos       0.99      0.41      0.58     11484\n",
            "       Probe       0.97      0.06      0.11      2947\n",
            "         R2L       0.03      1.00      0.06       274\n",
            "         U2R       0.00      0.00      0.00        15\n",
            "      normal       0.49      0.53      0.51     16774\n",
            "\n",
            "    accuracy                           0.45     31494\n",
            "   macro avg       0.50      0.40      0.25     31494\n",
            "weighted avg       0.71      0.45      0.50     31494\n",
            "\n",
            "Mean Absolute Error -  1.5958912808788976\n",
            "Mean Squared Error -  5.087540483901695\n",
            "Root Mean Squared Error -  2.2555576880012835\n",
            "R2 Score -  -30.86968742624783\n",
            "Accuracy -  44.66882580809043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceptron"
      ],
      "metadata": {
        "id": "xp7NTKJ6vxiy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "adam"
      ],
      "metadata": {
        "id": "wvN5V_0GwqnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = multi_data.iloc[:,0:93]  # dataset excluding target attribute (encoded, one-hot-encoded,original)\n",
        "Y = multi_data[['Dos','normal','Probe','R2L','U2R']] # target attributes\n",
        "\n",
        "# splitting the dataset 75% for training and 25% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.25, random_state=42)\n",
        "\n",
        "mlp = Sequential() # initializing model\n",
        "# input layer and first layer with 50 neurons\n",
        "mlp.add(Dense(units=50, input_dim=X_train.shape[1], activation='relu'))\n",
        "# output layer with softmax activation\n",
        "mlp.add(Dense(units=5,activation='softmax'))\n",
        "\n",
        "# defining loss function, optimizer, metrics and then compiling model\n",
        "mlp.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# summary of model layers\n",
        "mlp.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t4ALDfUv0mr",
        "outputId": "19fe1a1b-f78f-4c91-a8ea-9d3133992a77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 50)                4700      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 255       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,955\n",
            "Trainable params: 4,955\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model on training dataset\n",
        "history = mlp.fit(X_train, y_train, epochs=100, batch_size=5000,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYQLdWbFvHmL",
        "outputId": "cac0dfdc-27f9-4d44-92fb-8aa99ce7b42a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 2s 56ms/step - loss: 1.3571 - accuracy: 0.5723 - val_loss: 1.0658 - val_accuracy: 0.8028\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.8917 - accuracy: 0.8059 - val_loss: 0.7126 - val_accuracy: 0.8120\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 0.6165 - accuracy: 0.8183 - val_loss: 0.5220 - val_accuracy: 0.8484\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.4722 - accuracy: 0.8551 - val_loss: 0.4234 - val_accuracy: 0.8617\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.3937 - accuracy: 0.8631 - val_loss: 0.3656 - val_accuracy: 0.8773\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 0.3446 - accuracy: 0.8906 - val_loss: 0.3266 - val_accuracy: 0.9038\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.3093 - accuracy: 0.9103 - val_loss: 0.2966 - val_accuracy: 0.9156\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.2817 - accuracy: 0.9212 - val_loss: 0.2723 - val_accuracy: 0.9258\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.2594 - accuracy: 0.9283 - val_loss: 0.2524 - val_accuracy: 0.9310\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.2411 - accuracy: 0.9328 - val_loss: 0.2360 - val_accuracy: 0.9350\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.2259 - accuracy: 0.9376 - val_loss: 0.2222 - val_accuracy: 0.9416\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.2134 - accuracy: 0.9445 - val_loss: 0.2106 - val_accuracy: 0.9450\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.2026 - accuracy: 0.9466 - val_loss: 0.2006 - val_accuracy: 0.9462\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1934 - accuracy: 0.9473 - val_loss: 0.1919 - val_accuracy: 0.9469\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1854 - accuracy: 0.9480 - val_loss: 0.1845 - val_accuracy: 0.9476\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1784 - accuracy: 0.9487 - val_loss: 0.1778 - val_accuracy: 0.9482\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1723 - accuracy: 0.9496 - val_loss: 0.1720 - val_accuracy: 0.9500\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1669 - accuracy: 0.9508 - val_loss: 0.1668 - val_accuracy: 0.9503\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1621 - accuracy: 0.9510 - val_loss: 0.1622 - val_accuracy: 0.9506\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1578 - accuracy: 0.9513 - val_loss: 0.1581 - val_accuracy: 0.9507\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1539 - accuracy: 0.9515 - val_loss: 0.1543 - val_accuracy: 0.9508\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1504 - accuracy: 0.9516 - val_loss: 0.1508 - val_accuracy: 0.9509\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1472 - accuracy: 0.9516 - val_loss: 0.1476 - val_accuracy: 0.9511\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1442 - accuracy: 0.9518 - val_loss: 0.1447 - val_accuracy: 0.9512\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1414 - accuracy: 0.9520 - val_loss: 0.1419 - val_accuracy: 0.9515\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1388 - accuracy: 0.9525 - val_loss: 0.1395 - val_accuracy: 0.9519\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1364 - accuracy: 0.9530 - val_loss: 0.1370 - val_accuracy: 0.9526\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1341 - accuracy: 0.9532 - val_loss: 0.1349 - val_accuracy: 0.9531\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1320 - accuracy: 0.9537 - val_loss: 0.1328 - val_accuracy: 0.9535\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1300 - accuracy: 0.9544 - val_loss: 0.1309 - val_accuracy: 0.9534\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1281 - accuracy: 0.9547 - val_loss: 0.1291 - val_accuracy: 0.9539\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1264 - accuracy: 0.9549 - val_loss: 0.1274 - val_accuracy: 0.9542\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1248 - accuracy: 0.9552 - val_loss: 0.1257 - val_accuracy: 0.9546\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1232 - accuracy: 0.9560 - val_loss: 0.1243 - val_accuracy: 0.9556\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1218 - accuracy: 0.9565 - val_loss: 0.1228 - val_accuracy: 0.9561\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.1204 - accuracy: 0.9568 - val_loss: 0.1214 - val_accuracy: 0.9567\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1191 - accuracy: 0.9572 - val_loss: 0.1202 - val_accuracy: 0.9568\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1179 - accuracy: 0.9576 - val_loss: 0.1190 - val_accuracy: 0.9574\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1167 - accuracy: 0.9582 - val_loss: 0.1178 - val_accuracy: 0.9580\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1156 - accuracy: 0.9583 - val_loss: 0.1167 - val_accuracy: 0.9582\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1145 - accuracy: 0.9590 - val_loss: 0.1157 - val_accuracy: 0.9588\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1135 - accuracy: 0.9589 - val_loss: 0.1147 - val_accuracy: 0.9593\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1125 - accuracy: 0.9607 - val_loss: 0.1137 - val_accuracy: 0.9603\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1115 - accuracy: 0.9613 - val_loss: 0.1128 - val_accuracy: 0.9605\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1106 - accuracy: 0.9613 - val_loss: 0.1121 - val_accuracy: 0.9607\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1098 - accuracy: 0.9620 - val_loss: 0.1112 - val_accuracy: 0.9613\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1089 - accuracy: 0.9628 - val_loss: 0.1104 - val_accuracy: 0.9624\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1082 - accuracy: 0.9635 - val_loss: 0.1096 - val_accuracy: 0.9628\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1074 - accuracy: 0.9638 - val_loss: 0.1089 - val_accuracy: 0.9627\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1067 - accuracy: 0.9638 - val_loss: 0.1082 - val_accuracy: 0.9628\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1060 - accuracy: 0.9638 - val_loss: 0.1075 - val_accuracy: 0.9635\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1054 - accuracy: 0.9644 - val_loss: 0.1068 - val_accuracy: 0.9636\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1047 - accuracy: 0.9644 - val_loss: 0.1063 - val_accuracy: 0.9640\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1041 - accuracy: 0.9646 - val_loss: 0.1057 - val_accuracy: 0.9639\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1036 - accuracy: 0.9646 - val_loss: 0.1053 - val_accuracy: 0.9640\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1030 - accuracy: 0.9652 - val_loss: 0.1045 - val_accuracy: 0.9646\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1025 - accuracy: 0.9653 - val_loss: 0.1041 - val_accuracy: 0.9645\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1019 - accuracy: 0.9656 - val_loss: 0.1036 - val_accuracy: 0.9648\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1015 - accuracy: 0.9658 - val_loss: 0.1031 - val_accuracy: 0.9652\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1010 - accuracy: 0.9658 - val_loss: 0.1026 - val_accuracy: 0.9655\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1005 - accuracy: 0.9658 - val_loss: 0.1022 - val_accuracy: 0.9655\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0999 - accuracy: 0.9663 - val_loss: 0.1016 - val_accuracy: 0.9658\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0995 - accuracy: 0.9668 - val_loss: 0.1012 - val_accuracy: 0.9658\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0990 - accuracy: 0.9670 - val_loss: 0.1006 - val_accuracy: 0.9663\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0986 - accuracy: 0.9670 - val_loss: 0.1004 - val_accuracy: 0.9660\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0982 - accuracy: 0.9670 - val_loss: 0.0999 - val_accuracy: 0.9660\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0978 - accuracy: 0.9670 - val_loss: 0.0995 - val_accuracy: 0.9660\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0973 - accuracy: 0.9671 - val_loss: 0.0991 - val_accuracy: 0.9659\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0970 - accuracy: 0.9671 - val_loss: 0.0989 - val_accuracy: 0.9662\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0967 - accuracy: 0.9675 - val_loss: 0.0986 - val_accuracy: 0.9659\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0962 - accuracy: 0.9671 - val_loss: 0.0981 - val_accuracy: 0.9665\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0959 - accuracy: 0.9674 - val_loss: 0.0980 - val_accuracy: 0.9663\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0955 - accuracy: 0.9673 - val_loss: 0.0974 - val_accuracy: 0.9665\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0952 - accuracy: 0.9676 - val_loss: 0.0973 - val_accuracy: 0.9665\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0949 - accuracy: 0.9677 - val_loss: 0.0968 - val_accuracy: 0.9669\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0946 - accuracy: 0.9682 - val_loss: 0.0966 - val_accuracy: 0.9667\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0944 - accuracy: 0.9676 - val_loss: 0.0962 - val_accuracy: 0.9671\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0941 - accuracy: 0.9683 - val_loss: 0.0960 - val_accuracy: 0.9669\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0936 - accuracy: 0.9678 - val_loss: 0.0955 - val_accuracy: 0.9672\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0933 - accuracy: 0.9682 - val_loss: 0.0954 - val_accuracy: 0.9670\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0929 - accuracy: 0.9682 - val_loss: 0.0950 - val_accuracy: 0.9672\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0927 - accuracy: 0.9682 - val_loss: 0.0947 - val_accuracy: 0.9673\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0924 - accuracy: 0.9682 - val_loss: 0.0945 - val_accuracy: 0.9673\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0921 - accuracy: 0.9687 - val_loss: 0.0944 - val_accuracy: 0.9672\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0918 - accuracy: 0.9684 - val_loss: 0.0938 - val_accuracy: 0.9673\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0916 - accuracy: 0.9687 - val_loss: 0.0937 - val_accuracy: 0.9673\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0913 - accuracy: 0.9688 - val_loss: 0.0934 - val_accuracy: 0.9676\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0910 - accuracy: 0.9687 - val_loss: 0.0932 - val_accuracy: 0.9677\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0908 - accuracy: 0.9686 - val_loss: 0.0928 - val_accuracy: 0.9679\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0905 - accuracy: 0.9687 - val_loss: 0.0928 - val_accuracy: 0.9677\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0902 - accuracy: 0.9689 - val_loss: 0.0925 - val_accuracy: 0.9681\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0901 - accuracy: 0.9687 - val_loss: 0.0921 - val_accuracy: 0.9677\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0898 - accuracy: 0.9692 - val_loss: 0.0919 - val_accuracy: 0.9681\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0895 - accuracy: 0.9693 - val_loss: 0.0918 - val_accuracy: 0.9678\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0893 - accuracy: 0.9692 - val_loss: 0.0914 - val_accuracy: 0.9680\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0891 - accuracy: 0.9691 - val_loss: 0.0913 - val_accuracy: 0.9680\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0888 - accuracy: 0.9693 - val_loss: 0.0910 - val_accuracy: 0.9680\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0886 - accuracy: 0.9696 - val_loss: 0.0908 - val_accuracy: 0.9680\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0884 - accuracy: 0.9692 - val_loss: 0.0905 - val_accuracy: 0.9685\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0882 - accuracy: 0.9695 - val_loss: 0.0905 - val_accuracy: 0.9680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining loss function, optimizer, metrics and then compiling model\n",
        "mlp.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# predicting target attribute on testing dataset\n",
        "test_results = mlp.evaluate(X_test, y_test, verbose=1)\n",
        "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}%')\n",
        "\n",
        "pred = mlp.predict(X_test)\n",
        "\n",
        "for j in range(0,pred.shape[1]):\n",
        "  for i in range(0,pred.shape[0]):\n",
        "    pred[i][j] = int(round(pred[i][j]))\n",
        "\n",
        "pred_df = pd.DataFrame(pred,columns=y_test.columns)\n",
        "\n",
        "print(\"Recall Score - \",recall_score(y_test,pred_df.astype('uint8'),average='micro'))\n",
        "print(\"F1 Score - \",f1_score(y_test,pred_df.astype('uint8'),average='micro'))\n",
        "print(\"Precision Score - \",precision_score(y_test,pred_df.astype('uint8'),average='micro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcZQs9J4vHdr",
        "outputId": "0dde0493-9225-4e82-b5fd-e256465f02a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "985/985 [==============================] - 3s 2ms/step - loss: 0.0907 - accuracy: 0.9680\n",
            "Test results - Loss: 0.09065956622362137 - Accuracy: 96.79621458053589%\n",
            "Recall Score -  0.9666920683304756\n",
            "F1 Score -  0.968521847015222\n",
            "Precision Score -  0.9703585657370518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSPROP"
      ],
      "metadata": {
        "id": "NXg-k8oEwsr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = multi_data.iloc[:,0:93]  # dataset excluding target attribute (encoded, one-hot-encoded,original)\n",
        "Y = multi_data[['Dos','normal','Probe','R2L','U2R']] # target attributes\n",
        "\n",
        "# splitting the dataset 75% for training and 25% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.25, random_state=42)\n",
        "\n",
        "mlp = Sequential() # initializing model\n",
        "# input layer and first layer with 50 neurons\n",
        "mlp.add(Dense(units=50, input_dim=X_train.shape[1], activation='relu'))\n",
        "# output layer with softmax activation\n",
        "mlp.add(Dense(units=5,activation='softmax'))\n",
        "\n",
        "# defining loss function, optimizer, metrics and then compiling model\n",
        "mlp.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "# summary of model layers\n",
        "mlp.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wsursH2wuuB",
        "outputId": "fff60cc6-daf4-426e-eec9-820c1744ba83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (None, 50)                4700      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5)                 255       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,955\n",
            "Trainable params: 4,955\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model on training dataset\n",
        "history = mlp.fit(X_train, y_train, epochs=100, batch_size=5000,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYSwRHQWvHQT",
        "outputId": "ce10824e-1083-48a1-db0a-ef8d0685f8c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 1s 29ms/step - loss: 1.3664 - accuracy: 0.5807 - val_loss: 0.9740 - val_accuracy: 0.8649\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.8052 - accuracy: 0.8644 - val_loss: 0.6372 - val_accuracy: 0.8681\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.5515 - accuracy: 0.8675 - val_loss: 0.4648 - val_accuracy: 0.8739\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.4177 - accuracy: 0.8797 - val_loss: 0.3696 - val_accuracy: 0.8961\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.3400 - accuracy: 0.9005 - val_loss: 0.3120 - val_accuracy: 0.9106\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.2902 - accuracy: 0.9148 - val_loss: 0.2722 - val_accuracy: 0.9258\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.2546 - accuracy: 0.9267 - val_loss: 0.2425 - val_accuracy: 0.9356\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.2275 - accuracy: 0.9375 - val_loss: 0.2186 - val_accuracy: 0.9413\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.2061 - accuracy: 0.9437 - val_loss: 0.1994 - val_accuracy: 0.9453\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1887 - accuracy: 0.9471 - val_loss: 0.1831 - val_accuracy: 0.9479\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1741 - accuracy: 0.9490 - val_loss: 0.1697 - val_accuracy: 0.9500\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1623 - accuracy: 0.9508 - val_loss: 0.1588 - val_accuracy: 0.9507\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1526 - accuracy: 0.9521 - val_loss: 0.1498 - val_accuracy: 0.9519\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1445 - accuracy: 0.9530 - val_loss: 0.1423 - val_accuracy: 0.9520\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.1377 - accuracy: 0.9533 - val_loss: 0.1365 - val_accuracy: 0.9531\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1320 - accuracy: 0.9536 - val_loss: 0.1312 - val_accuracy: 0.9532\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1271 - accuracy: 0.9547 - val_loss: 0.1259 - val_accuracy: 0.9551\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1228 - accuracy: 0.9561 - val_loss: 0.1222 - val_accuracy: 0.9563\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1192 - accuracy: 0.9572 - val_loss: 0.1181 - val_accuracy: 0.9567\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1159 - accuracy: 0.9583 - val_loss: 0.1156 - val_accuracy: 0.9592\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1131 - accuracy: 0.9594 - val_loss: 0.1125 - val_accuracy: 0.9604\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.1104 - accuracy: 0.9613 - val_loss: 0.1099 - val_accuracy: 0.9630\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1081 - accuracy: 0.9637 - val_loss: 0.1077 - val_accuracy: 0.9625\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1059 - accuracy: 0.9644 - val_loss: 0.1058 - val_accuracy: 0.9648\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.1039 - accuracy: 0.9654 - val_loss: 0.1044 - val_accuracy: 0.9648\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.1021 - accuracy: 0.9659 - val_loss: 0.1036 - val_accuracy: 0.9655\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.1006 - accuracy: 0.9663 - val_loss: 0.1009 - val_accuracy: 0.9659\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0990 - accuracy: 0.9668 - val_loss: 0.0997 - val_accuracy: 0.9648\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0978 - accuracy: 0.9668 - val_loss: 0.0986 - val_accuracy: 0.9673\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0965 - accuracy: 0.9675 - val_loss: 0.0974 - val_accuracy: 0.9661\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0954 - accuracy: 0.9680 - val_loss: 0.0965 - val_accuracy: 0.9675\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0944 - accuracy: 0.9684 - val_loss: 0.0958 - val_accuracy: 0.9678\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0934 - accuracy: 0.9687 - val_loss: 0.0944 - val_accuracy: 0.9676\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0926 - accuracy: 0.9691 - val_loss: 0.0937 - val_accuracy: 0.9681\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0917 - accuracy: 0.9689 - val_loss: 0.0928 - val_accuracy: 0.9682\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0908 - accuracy: 0.9696 - val_loss: 0.0923 - val_accuracy: 0.9696\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0901 - accuracy: 0.9701 - val_loss: 0.0917 - val_accuracy: 0.9691\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0894 - accuracy: 0.9698 - val_loss: 0.0917 - val_accuracy: 0.9681\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0888 - accuracy: 0.9704 - val_loss: 0.0903 - val_accuracy: 0.9696\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0879 - accuracy: 0.9707 - val_loss: 0.0894 - val_accuracy: 0.9703\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0874 - accuracy: 0.9708 - val_loss: 0.0899 - val_accuracy: 0.9702\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0868 - accuracy: 0.9711 - val_loss: 0.0886 - val_accuracy: 0.9706\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0863 - accuracy: 0.9713 - val_loss: 0.0887 - val_accuracy: 0.9708\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0858 - accuracy: 0.9715 - val_loss: 0.0877 - val_accuracy: 0.9700\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0851 - accuracy: 0.9716 - val_loss: 0.0873 - val_accuracy: 0.9701\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0848 - accuracy: 0.9719 - val_loss: 0.0868 - val_accuracy: 0.9702\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0842 - accuracy: 0.9721 - val_loss: 0.0855 - val_accuracy: 0.9717\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0838 - accuracy: 0.9725 - val_loss: 0.0849 - val_accuracy: 0.9713\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0833 - accuracy: 0.9726 - val_loss: 0.0851 - val_accuracy: 0.9718\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0828 - accuracy: 0.9727 - val_loss: 0.0849 - val_accuracy: 0.9707\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0825 - accuracy: 0.9726 - val_loss: 0.0858 - val_accuracy: 0.9723\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0820 - accuracy: 0.9729 - val_loss: 0.0836 - val_accuracy: 0.9714\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0816 - accuracy: 0.9732 - val_loss: 0.0836 - val_accuracy: 0.9705\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0815 - accuracy: 0.9731 - val_loss: 0.0829 - val_accuracy: 0.9730\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0811 - accuracy: 0.9732 - val_loss: 0.0821 - val_accuracy: 0.9724\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0805 - accuracy: 0.9734 - val_loss: 0.0831 - val_accuracy: 0.9738\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0803 - accuracy: 0.9737 - val_loss: 0.0832 - val_accuracy: 0.9733\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0800 - accuracy: 0.9738 - val_loss: 0.0811 - val_accuracy: 0.9728\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0797 - accuracy: 0.9737 - val_loss: 0.0812 - val_accuracy: 0.9743\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0793 - accuracy: 0.9743 - val_loss: 0.0822 - val_accuracy: 0.9739\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0790 - accuracy: 0.9742 - val_loss: 0.0814 - val_accuracy: 0.9722\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0788 - accuracy: 0.9739 - val_loss: 0.0810 - val_accuracy: 0.9745\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0784 - accuracy: 0.9744 - val_loss: 0.0797 - val_accuracy: 0.9735\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0781 - accuracy: 0.9744 - val_loss: 0.0797 - val_accuracy: 0.9731\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0777 - accuracy: 0.9742 - val_loss: 0.0792 - val_accuracy: 0.9742\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0775 - accuracy: 0.9747 - val_loss: 0.0797 - val_accuracy: 0.9741\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0772 - accuracy: 0.9745 - val_loss: 0.0789 - val_accuracy: 0.9731\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0770 - accuracy: 0.9746 - val_loss: 0.0789 - val_accuracy: 0.9733\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0768 - accuracy: 0.9748 - val_loss: 0.0782 - val_accuracy: 0.9740\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0765 - accuracy: 0.9749 - val_loss: 0.0793 - val_accuracy: 0.9749\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0762 - accuracy: 0.9753 - val_loss: 0.0780 - val_accuracy: 0.9747\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0759 - accuracy: 0.9754 - val_loss: 0.0780 - val_accuracy: 0.9741\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0757 - accuracy: 0.9752 - val_loss: 0.0773 - val_accuracy: 0.9740\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0755 - accuracy: 0.9752 - val_loss: 0.0781 - val_accuracy: 0.9753\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0753 - accuracy: 0.9757 - val_loss: 0.0768 - val_accuracy: 0.9741\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0749 - accuracy: 0.9753 - val_loss: 0.0773 - val_accuracy: 0.9743\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0749 - accuracy: 0.9753 - val_loss: 0.0769 - val_accuracy: 0.9737\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0746 - accuracy: 0.9757 - val_loss: 0.0770 - val_accuracy: 0.9745\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.0743 - accuracy: 0.9759 - val_loss: 0.0766 - val_accuracy: 0.9741\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0741 - accuracy: 0.9754 - val_loss: 0.0762 - val_accuracy: 0.9753\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0740 - accuracy: 0.9758 - val_loss: 0.0764 - val_accuracy: 0.9754\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0737 - accuracy: 0.9760 - val_loss: 0.0758 - val_accuracy: 0.9742\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0737 - accuracy: 0.9759 - val_loss: 0.0759 - val_accuracy: 0.9746\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0734 - accuracy: 0.9755 - val_loss: 0.0753 - val_accuracy: 0.9749\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0732 - accuracy: 0.9757 - val_loss: 0.0748 - val_accuracy: 0.9752\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0730 - accuracy: 0.9762 - val_loss: 0.0754 - val_accuracy: 0.9745\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0728 - accuracy: 0.9760 - val_loss: 0.0750 - val_accuracy: 0.9753\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0726 - accuracy: 0.9762 - val_loss: 0.0752 - val_accuracy: 0.9757\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0724 - accuracy: 0.9763 - val_loss: 0.0749 - val_accuracy: 0.9759\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.0722 - accuracy: 0.9762 - val_loss: 0.0742 - val_accuracy: 0.9756\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0721 - accuracy: 0.9763 - val_loss: 0.0749 - val_accuracy: 0.9742\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.0719 - accuracy: 0.9765 - val_loss: 0.0747 - val_accuracy: 0.9760\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0716 - accuracy: 0.9765 - val_loss: 0.0743 - val_accuracy: 0.9756\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0715 - accuracy: 0.9764 - val_loss: 0.0740 - val_accuracy: 0.9757\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0713 - accuracy: 0.9767 - val_loss: 0.0741 - val_accuracy: 0.9765\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0713 - accuracy: 0.9766 - val_loss: 0.0740 - val_accuracy: 0.9763\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0710 - accuracy: 0.9770 - val_loss: 0.0731 - val_accuracy: 0.9756\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0708 - accuracy: 0.9768 - val_loss: 0.0729 - val_accuracy: 0.9763\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0707 - accuracy: 0.9768 - val_loss: 0.0728 - val_accuracy: 0.9761\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.0705 - accuracy: 0.9772 - val_loss: 0.0739 - val_accuracy: 0.9761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining loss function, optimizer, metrics and then compiling model\n",
        "mlp.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "# predicting target attribute on testing dataset\n",
        "test_results = mlp.evaluate(X_test, y_test, verbose=1)\n",
        "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}%')\n",
        "\n",
        "pred = mlp.predict(X_test)\n",
        "\n",
        "for j in range(0,pred.shape[1]):\n",
        "  for i in range(0,pred.shape[0]):\n",
        "    pred[i][j] = int(round(pred[i][j]))\n",
        "\n",
        "pred_df = pd.DataFrame(pred,columns=y_test.columns)\n",
        "\n",
        "print(\"Recall Score - \",recall_score(y_test,pred_df.astype('uint8'),average='micro'))\n",
        "print(\"F1 Score - \",f1_score(y_test,pred_df.astype('uint8'),average='micro'))\n",
        "print(\"Precision Score - \",precision_score(y_test,pred_df.astype('uint8'),average='micro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "785HUFt_w3Nr",
        "outputId": "0884c456-ad85-4377-f569-04e27260044f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "985/985 [==============================] - 2s 1ms/step - loss: 0.0750 - accuracy: 0.9764\n",
            "Test results - Loss: 0.07497596740722656 - Accuracy: 97.64081835746765%\n",
            "Recall Score -  0.9752968819457675\n",
            "F1 Score -  0.9764750762970498\n",
            "Precision Score -  0.977656120695143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ada-grad"
      ],
      "metadata": {
        "id": "1rWXerUpxICS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = multi_data.iloc[:,0:93]  # dataset excluding target attribute (encoded, one-hot-encoded,original)\n",
        "Y = multi_data[['Dos','normal','Probe','R2L','U2R']] # target attributes\n",
        "\n",
        "# splitting the dataset 75% for training and 25% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.25, random_state=42)\n",
        "\n",
        "mlp = Sequential() # initializing model\n",
        "# input layer and first layer with 50 neurons\n",
        "mlp.add(Dense(units=50, input_dim=X_train.shape[1], activation='relu'))\n",
        "# output layer with softmax activation\n",
        "mlp.add(Dense(units=5,activation='softmax'))\n",
        "\n",
        "# defining loss function, optimizer, metrics and then compiling model\n",
        "mlp.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
        "\n",
        "# summary of model layers\n",
        "mlp.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7Wf4Pn6xHjL",
        "outputId": "7ec56af3-e8ad-46ab-e9ef-0a6e317e2d00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_4 (Dense)             (None, 50)                4700      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 5)                 255       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,955\n",
            "Trainable params: 4,955\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training the model on training dataset\n",
        "history = mlp.fit(X_train, y_train, epochs=100, batch_size=5000,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkBYCMaCxHXn",
        "outputId": "f966ed39-5fa5-4baf-ac28-2d5b11f464f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 3s 72ms/step - loss: 1.9723 - accuracy: 0.0078 - val_loss: 1.8867 - val_accuracy: 0.0089\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 1.8354 - accuracy: 0.0093 - val_loss: 1.7758 - val_accuracy: 0.0101\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 1.7360 - accuracy: 0.0123 - val_loss: 1.6878 - val_accuracy: 0.0173\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.6546 - accuracy: 0.0225 - val_loss: 1.6138 - val_accuracy: 0.0289\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 1.5850 - accuracy: 0.0358 - val_loss: 1.5489 - val_accuracy: 0.0500\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 0s 28ms/step - loss: 1.5235 - accuracy: 0.0726 - val_loss: 1.4916 - val_accuracy: 0.1044\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 1.4690 - accuracy: 0.1521 - val_loss: 1.4404 - val_accuracy: 0.2320\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 1.4202 - accuracy: 0.4705 - val_loss: 1.3942 - val_accuracy: 0.5045\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.3757 - accuracy: 0.5224 - val_loss: 1.3515 - val_accuracy: 0.5451\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 1.3344 - accuracy: 0.5750 - val_loss: 1.3120 - val_accuracy: 0.6131\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 0s 23ms/step - loss: 1.2962 - accuracy: 0.6392 - val_loss: 1.2752 - val_accuracy: 0.6726\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2605 - accuracy: 0.6887 - val_loss: 1.2407 - val_accuracy: 0.7180\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 1.2266 - accuracy: 0.7310 - val_loss: 1.2077 - val_accuracy: 0.7520\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 1.1942 - accuracy: 0.7592 - val_loss: 1.1760 - val_accuracy: 0.7713\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 1.1634 - accuracy: 0.7761 - val_loss: 1.1462 - val_accuracy: 0.7851\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 1.1344 - accuracy: 0.7897 - val_loss: 1.1182 - val_accuracy: 0.7978\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 21ms/step - loss: 1.1071 - accuracy: 0.8005 - val_loss: 1.0917 - val_accuracy: 0.8058\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 1.0813 - accuracy: 0.8078 - val_loss: 1.0667 - val_accuracy: 0.8133\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 1.0569 - accuracy: 0.8159 - val_loss: 1.0430 - val_accuracy: 0.8223\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 1.0337 - accuracy: 0.8236 - val_loss: 1.0205 - val_accuracy: 0.8283\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 1.0117 - accuracy: 0.8307 - val_loss: 0.9992 - val_accuracy: 0.8344\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.9909 - accuracy: 0.8370 - val_loss: 0.9789 - val_accuracy: 0.8405\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.9710 - accuracy: 0.8412 - val_loss: 0.9596 - val_accuracy: 0.8440\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.9522 - accuracy: 0.8436 - val_loss: 0.9412 - val_accuracy: 0.8464\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.9342 - accuracy: 0.8456 - val_loss: 0.9237 - val_accuracy: 0.8479\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.9170 - accuracy: 0.8465 - val_loss: 0.9070 - val_accuracy: 0.8495\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.9006 - accuracy: 0.8480 - val_loss: 0.8911 - val_accuracy: 0.8510\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.8850 - accuracy: 0.8489 - val_loss: 0.8758 - val_accuracy: 0.8525\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.8700 - accuracy: 0.8501 - val_loss: 0.8611 - val_accuracy: 0.8531\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.8556 - accuracy: 0.8509 - val_loss: 0.8471 - val_accuracy: 0.8541\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.8419 - accuracy: 0.8517 - val_loss: 0.8338 - val_accuracy: 0.8550\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.8288 - accuracy: 0.8527 - val_loss: 0.8210 - val_accuracy: 0.8562\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.8162 - accuracy: 0.8535 - val_loss: 0.8087 - val_accuracy: 0.8568\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.8041 - accuracy: 0.8542 - val_loss: 0.7969 - val_accuracy: 0.8575\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.7926 - accuracy: 0.8545 - val_loss: 0.7855 - val_accuracy: 0.8578\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.7814 - accuracy: 0.8551 - val_loss: 0.7746 - val_accuracy: 0.8584\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.7707 - accuracy: 0.8558 - val_loss: 0.7641 - val_accuracy: 0.8586\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.7604 - accuracy: 0.8562 - val_loss: 0.7540 - val_accuracy: 0.8589\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.7505 - accuracy: 0.8564 - val_loss: 0.7444 - val_accuracy: 0.8592\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.7410 - accuracy: 0.8567 - val_loss: 0.7351 - val_accuracy: 0.8595\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.7319 - accuracy: 0.8569 - val_loss: 0.7262 - val_accuracy: 0.8598\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.7231 - accuracy: 0.8572 - val_loss: 0.7174 - val_accuracy: 0.8604\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.7144 - accuracy: 0.8577 - val_loss: 0.7089 - val_accuracy: 0.8610\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.7060 - accuracy: 0.8583 - val_loss: 0.7007 - val_accuracy: 0.8615\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.6979 - accuracy: 0.8589 - val_loss: 0.6927 - val_accuracy: 0.8624\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.6900 - accuracy: 0.8595 - val_loss: 0.6850 - val_accuracy: 0.8633\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.6825 - accuracy: 0.8604 - val_loss: 0.6776 - val_accuracy: 0.8641\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.6752 - accuracy: 0.8616 - val_loss: 0.6705 - val_accuracy: 0.8647\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.6682 - accuracy: 0.8621 - val_loss: 0.6637 - val_accuracy: 0.8648\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.6614 - accuracy: 0.8625 - val_loss: 0.6571 - val_accuracy: 0.8654\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.6549 - accuracy: 0.8631 - val_loss: 0.6507 - val_accuracy: 0.8662\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.6486 - accuracy: 0.8638 - val_loss: 0.6445 - val_accuracy: 0.8673\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.6425 - accuracy: 0.8648 - val_loss: 0.6385 - val_accuracy: 0.8677\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.6366 - accuracy: 0.8654 - val_loss: 0.6327 - val_accuracy: 0.8682\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.6309 - accuracy: 0.8660 - val_loss: 0.6271 - val_accuracy: 0.8685\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.6254 - accuracy: 0.8664 - val_loss: 0.6217 - val_accuracy: 0.8693\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.6200 - accuracy: 0.8668 - val_loss: 0.6165 - val_accuracy: 0.8698\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.6148 - accuracy: 0.8671 - val_loss: 0.6114 - val_accuracy: 0.8702\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.6098 - accuracy: 0.8676 - val_loss: 0.6064 - val_accuracy: 0.8708\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.6049 - accuracy: 0.8681 - val_loss: 0.6016 - val_accuracy: 0.8712\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.6002 - accuracy: 0.8686 - val_loss: 0.5970 - val_accuracy: 0.8715\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.5956 - accuracy: 0.8687 - val_loss: 0.5924 - val_accuracy: 0.8713\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.5911 - accuracy: 0.8686 - val_loss: 0.5880 - val_accuracy: 0.8713\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.5867 - accuracy: 0.8686 - val_loss: 0.5837 - val_accuracy: 0.8712\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.5824 - accuracy: 0.8686 - val_loss: 0.5795 - val_accuracy: 0.8713\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.5783 - accuracy: 0.8687 - val_loss: 0.5754 - val_accuracy: 0.8713\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.5743 - accuracy: 0.8688 - val_loss: 0.5715 - val_accuracy: 0.8714\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.5703 - accuracy: 0.8688 - val_loss: 0.5676 - val_accuracy: 0.8715\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.5665 - accuracy: 0.8689 - val_loss: 0.5639 - val_accuracy: 0.8714\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.5628 - accuracy: 0.8689 - val_loss: 0.5602 - val_accuracy: 0.8714\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.5592 - accuracy: 0.8689 - val_loss: 0.5566 - val_accuracy: 0.8713\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.5556 - accuracy: 0.8690 - val_loss: 0.5532 - val_accuracy: 0.8714\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.5522 - accuracy: 0.8690 - val_loss: 0.5498 - val_accuracy: 0.8714\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.5488 - accuracy: 0.8690 - val_loss: 0.5465 - val_accuracy: 0.8714\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.5456 - accuracy: 0.8690 - val_loss: 0.5433 - val_accuracy: 0.8714\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.5424 - accuracy: 0.8690 - val_loss: 0.5402 - val_accuracy: 0.8714\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.5393 - accuracy: 0.8690 - val_loss: 0.5371 - val_accuracy: 0.8713\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.5362 - accuracy: 0.8690 - val_loss: 0.5341 - val_accuracy: 0.8713\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.5332 - accuracy: 0.8691 - val_loss: 0.5312 - val_accuracy: 0.8713\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.5303 - accuracy: 0.8691 - val_loss: 0.5283 - val_accuracy: 0.8714\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.5275 - accuracy: 0.8691 - val_loss: 0.5255 - val_accuracy: 0.8715\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.5247 - accuracy: 0.8691 - val_loss: 0.5228 - val_accuracy: 0.8715\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.5220 - accuracy: 0.8692 - val_loss: 0.5201 - val_accuracy: 0.8715\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.5194 - accuracy: 0.8692 - val_loss: 0.5175 - val_accuracy: 0.8716\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.5168 - accuracy: 0.8692 - val_loss: 0.5150 - val_accuracy: 0.8717\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.5142 - accuracy: 0.8692 - val_loss: 0.5125 - val_accuracy: 0.8717\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 16ms/step - loss: 0.5117 - accuracy: 0.8692 - val_loss: 0.5100 - val_accuracy: 0.8717\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.5093 - accuracy: 0.8693 - val_loss: 0.5076 - val_accuracy: 0.8717\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.5069 - accuracy: 0.8693 - val_loss: 0.5053 - val_accuracy: 0.8717\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.5046 - accuracy: 0.8693 - val_loss: 0.5030 - val_accuracy: 0.8718\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.5023 - accuracy: 0.8693 - val_loss: 0.5007 - val_accuracy: 0.8718\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.5000 - accuracy: 0.8693 - val_loss: 0.4985 - val_accuracy: 0.8718\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.4978 - accuracy: 0.8749 - val_loss: 0.4964 - val_accuracy: 0.8807\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.4957 - accuracy: 0.8785 - val_loss: 0.4942 - val_accuracy: 0.8810\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.4936 - accuracy: 0.8789 - val_loss: 0.4921 - val_accuracy: 0.8813\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.4915 - accuracy: 0.8791 - val_loss: 0.4901 - val_accuracy: 0.8814\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 15ms/step - loss: 0.4895 - accuracy: 0.8793 - val_loss: 0.4881 - val_accuracy: 0.8814\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.4875 - accuracy: 0.8793 - val_loss: 0.4862 - val_accuracy: 0.8814\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 14ms/step - loss: 0.4855 - accuracy: 0.8794 - val_loss: 0.4842 - val_accuracy: 0.8815\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 13ms/step - loss: 0.4836 - accuracy: 0.8794 - val_loss: 0.4823 - val_accuracy: 0.8815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining loss function, optimizer, metrics and then compiling model\n",
        "mlp.compile(loss='categorical_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
        "\n",
        "# predicting target attribute on testing dataset\n",
        "test_results = mlp.evaluate(X_test, y_test, verbose=1)\n",
        "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]*100}%')\n",
        "\n",
        "pred = mlp.predict(X_test)\n",
        "\n",
        "for j in range(0,pred.shape[1]):\n",
        "  for i in range(0,pred.shape[0]):\n",
        "    pred[i][j] = int(round(pred[i][j]))\n",
        "\n",
        "pred_df = pd.DataFrame(pred,columns=y_test.columns)\n",
        "\n",
        "print(\"Recall Score - \",recall_score(y_test,pred_df.astype('uint8'),average='micro'))\n",
        "print(\"F1 Score - \",f1_score(y_test,pred_df.astype('uint8'),average='micro'))\n",
        "print(\"Precision Score - \",precision_score(y_test,pred_df.astype('uint8'),average='micro'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHEo8xlFxSRS",
        "outputId": "b4f4eac9-28f6-4f75-f5db-002e1e40555f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "985/985 [==============================] - 2s 1ms/step - loss: 0.4872 - accuracy: 0.8773\n",
            "Test results - Loss: 0.4871964156627655 - Accuracy: 87.73416876792908%\n",
            "Recall Score -  0.8083126944814886\n",
            "F1 Score -  0.8670197367300717\n",
            "Precision Score -  0.9349223254618237\n"
          ]
        }
      ]
    }
  ]
}